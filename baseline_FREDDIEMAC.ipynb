{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC80fhtPwLAi"
      },
      "source": [
        "# 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "ZxLf7v9yYnot",
        "outputId": "a6107c00-b62b-421e-dd13-f8d2aa922f07"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/gdrive', force_remount=True)\n",
        "# SRC_PATH = '/content/gdrive/MyDrive/MP FEB/FREDDIEMAC'\n",
        "# DATA_SRC_PATH = SRC_PATH + \"/data\"\n",
        "# sys.path.append(SRC_PATH)\n",
        "\n",
        "# !pip install wandb -qqq\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "\n",
        "# !pip install dask[dataframe] -qqq\n",
        "# !pip install fastparquet python-snappy -qqq\n",
        "# import dask.dataframe as dd\n",
        "\n",
        "\n",
        "\n",
        "# wandb.init(\n",
        "#     # Set the project where this run will be logged\n",
        "#     project=\"baseline_FREDDIEMAC\", \n",
        "#     # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
        "#     #name=\"experiment 1\"\n",
        "#     # Track hyperparameters and run metadata\n",
        "#     #config={\n",
        "#       #\"learning_rate\": 0.02,\n",
        "#       #\"architecture\": \"CNN\",\n",
        "#       #\"dataset\": \"CIFAR-100\",\n",
        "#       #\"epochs\": 10,}\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_SRC_PATH = \"./data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_c8tByEYno1"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "DEVICE = 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLOBqrNZYno2"
      },
      "source": [
        "# 2. Import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUuVGUU9Yno4"
      },
      "source": [
        "### 2.1 Loading raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie6Sr0xeYno5"
      },
      "outputs": [],
      "source": [
        "blumenstock_types = {'LOAN_SEQUENCE_NUMBER': str, 'INT_RATE': float, 'ORIG_UPB': float, 'FICO_SCORE': float,\n",
        "                    'DTI_R': float, 'LTV_R': float, 'FIRST_PAYMENT_DATE': str, 'BAL_REPAID': float, 'T_ACT_12M': float, 'T_DEL_30D': float, \n",
        "                    'T_DEL_60D': float, 'LABEL': float, 'REMAINING_MONTHS_TO_LEGAL_MATURITY': float, \"TIME_TO_EVENT\": float, 'TOTAL_OBSERVED_LENGTH': float}\n",
        "\n",
        "df_blumenstock = dd.read_parquet(DATA_SRC_PATH + \"/blumenstock_labeled_sample_orig_*.parquet.gzip\", engine=\"fastparquet\")\n",
        "df_blumenstock = df_blumenstock.astype(blumenstock_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6L2FdapYno7"
      },
      "source": [
        "### 2.2 Normalising raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FKNVKDHYno8"
      },
      "outputs": [],
      "source": [
        "covariates_to_normalise = ['INT_RATE', 'ORIG_UPB', 'FICO_SCORE', 'DTI_R', 'LTV_R', 'REMAINING_MONTHS_TO_LEGAL_MATURITY',\n",
        "                           'BAL_REPAID', 'T_ACT_12M', 'T_DEL_30D', 'T_DEL_60D']\n",
        "\n",
        "df_blumenstoch_mean = df_blumenstock[covariates_to_normalise].mean().compute()\n",
        "df_blumenstoch_std = df_blumenstock[covariates_to_normalise].std().compute()\n",
        "\n",
        "df_blumenstock[covariates_to_normalise] = (df_blumenstock[covariates_to_normalise] - df_blumenstoch_mean) / df_blumenstoch_std\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oclnws3qfgUe"
      },
      "source": [
        "### 2.3 Splitting train, validation and test-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnjnJOto42TE",
        "outputId": "4901f7f1-2d0b-47ae-e9f3-77b2b271c5e1"
      },
      "outputs": [],
      "source": [
        "train_df_blumenstock, validate_df_blumenstock, test_df_blumenstock = df_blumenstock.random_split([0.9, 0.08, 0.02])\n",
        "print(\"Total train set contains %d loans\" % len(train_df_blumenstock))\n",
        "print(\"Total validation set contains %d loans\" % len(validate_df_blumenstock))\n",
        "print(\"Total test set contains %d loans\" % len(test_df_blumenstock))\n",
        "print(\"-------------------------------------------------------------\")\n",
        "\n",
        "AMOUNT_OF_TRAIN_SAMPLES = 2**14 + 1\n",
        "AMOUNT_OF_VALIDATE_SAMPLES = 2**8 + 1\n",
        "AMOUNT_OF_TEST_SAMPLES = 2**7 + 1\n",
        "\n",
        "total_amount_of_train_samples = len(train_df_blumenstock)\n",
        "total_amount_of_validate_samples = len(validate_df_blumenstock)\n",
        "total_amount_of_test_samples = len(test_df_blumenstock)\n",
        "\n",
        "train_df_blumenstock = train_df_blumenstock.sample(frac=AMOUNT_OF_TRAIN_SAMPLES/total_amount_of_train_samples, replace=None, random_state=2022)\n",
        "validate_df_blumenstock = validate_df_blumenstock.sample(frac=AMOUNT_OF_VALIDATE_SAMPLES/total_amount_of_validate_samples, replace=None, random_state=2022)\n",
        "test_df_blumenstock = test_df_blumenstock.sample(frac=AMOUNT_OF_TEST_SAMPLES/total_amount_of_test_samples, replace=None, random_state=2022)\n",
        "\n",
        "print(\"Train set contains %d loans\" % len(train_df_blumenstock))\n",
        "print(\"validation set contains %d loans\" % len(validate_df_blumenstock))\n",
        "print(\"Test set contains %d loans\" % len(test_df_blumenstock))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3EfnsUtYno_"
      },
      "source": [
        "### 2.4 Creating dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI-DtH6P8Rvd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from FREDDIEMAC_baseline_data import FREDDIEMAC_basline_dataset, FREDDIEMAC_baseline_dataloader\n",
        "\n",
        "BATCH_SIZE = 2**7\n",
        "\n",
        "allowed_covariates = ['INT_RATE', 'ORIG_UPB', 'FICO_SCORE', 'DTI_R','LTV_R', 'BAL_REPAID', \n",
        "                     'T_ACT_12M', 'T_DEL_30D', 'T_DEL_60D', 'REMAINING_MONTHS_TO_LEGAL_MATURITY']\n",
        "\n",
        "TIME_TO_EVENT_covariate ='TIME_TO_EVENT'\n",
        "LABEL_covariate = 'LABEL'\n",
        "random_state = 123\n",
        "data_augment_factor = 3\n",
        "\n",
        "print(\"batch_size = \", BATCH_SIZE)\n",
        "print(\"number of covariates = \", len(allowed_covariates))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOCpNjTl1yGG"
      },
      "source": [
        "#### 2.4.1 Creating train dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEzNOyJ0Yno_",
        "outputId": "5d29ec90-e64e-433c-8bea-1a768f1be5ee"
      },
      "outputs": [],
      "source": [
        "train_FREDDIEMAC_raw_dataset = FREDDIEMAC_basline_dataset(train_df_blumenstock, \n",
        "                                                          allowed_covariates,\n",
        "                                                          TIME_TO_EVENT_covariate,\n",
        "                                                          LABEL_covariate,\n",
        "                                                          frac_cases=1,\n",
        "                                                          random_state=random_state,\n",
        "                                                          test_set=False,\n",
        "                                                          augment=False,\n",
        "                                                          data_augment_factor=data_augment_factor)\n",
        "\n",
        "print(\"This dataset will contain %d samples\" % len(train_FREDDIEMAC_raw_dataset))\n",
        "train_data_loader = FREDDIEMAC_baseline_dataloader(dataset=train_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
        "print(\"This dataloader will deliver %d batches\" % train_data_loader.get_max_iterations())\n",
        "batch_data, batch_data_length, batch_event, batch_tte = next(train_data_loader)\n",
        "\n",
        "print(batch_data.shape)\n",
        "print(batch_data_length.shape)\n",
        "print(batch_event.shape)\n",
        "print(batch_tte.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDHuOj_Q12rB"
      },
      "source": [
        "#### 2.4.2 Creating validate dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzrXVBR-gVGk",
        "outputId": "c3585408-7b86-42e0-9536-02fad1e59dba"
      },
      "outputs": [],
      "source": [
        "validate_FREDDIEMAC_raw_dataset = FREDDIEMAC_basline_dataset(validate_df_blumenstock, \n",
        "                                                          allowed_covariates,\n",
        "                                                          TIME_TO_EVENT_covariate,\n",
        "                                                          LABEL_covariate,\n",
        "                                                          frac_cases=1,\n",
        "                                                          random_state=random_state,\n",
        "                                                          test_set=False,\n",
        "                                                          augment=False,\n",
        "                                                          data_augment_factor=data_augment_factor)\n",
        "\n",
        "print(\"This dataset will contain %d samples\" % len(validate_df_blumenstock))\n",
        "validate_data_loader = FREDDIEMAC_baseline_dataloader(dataset=validate_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
        "print(\"This dataloader will deliver %d batches\" % validate_data_loader.get_max_iterations())\n",
        "batch_data, batch_data_length, batch_event, batch_tte = next(validate_data_loader)\n",
        "\n",
        "print(batch_data.shape)\n",
        "print(batch_data_length.shape)\n",
        "print(batch_event.shape)\n",
        "print(batch_tte.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FnYb9y62WXO"
      },
      "source": [
        "#### 2.4.3 creating a test sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFeWjs5P2bpH",
        "outputId": "c594e7a9-0a5a-4775-c480-89aa8e700ff6"
      },
      "outputs": [],
      "source": [
        "test_FREDDIEMAC_raw_dataset = FREDDIEMAC_basline_dataset(test_df_blumenstock, \n",
        "                                                          allowed_covariates,\n",
        "                                                          TIME_TO_EVENT_covariate,\n",
        "                                                          LABEL_covariate,\n",
        "                                                          frac_cases=1,\n",
        "                                                          random_state=random_state,\n",
        "                                                          test_set=False,\n",
        "                                                          augment=False,\n",
        "                                                          data_augment_factor=data_augment_factor)\n",
        "\n",
        "print(\"This dataset will contain %d samples\" % len(test_df_blumenstock))\n",
        "test_data_loader = FREDDIEMAC_baseline_dataloader(dataset=test_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
        "print(\"This dataloader will deliver %d batches\" % test_data_loader.get_max_iterations())\n",
        "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(test_data_loader)\n",
        "\n",
        "print(test_batch_data.shape)\n",
        "print(test_batch_data_length.shape)\n",
        "print(test_batch_event.shape)\n",
        "print(test_batch_tte.shape)\n",
        "\n",
        "print(\"The first sample will look like this:\")\n",
        "print(\"batch event= %d --- batch_data_length= %d --- batch_tte= %d\" % (test_batch_event[0], test_batch_data_length[0], test_batch_tte[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJpXyyTQYnpB"
      },
      "source": [
        "### 2.5 Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtFNogjzYnpB",
        "outputId": "f79396fd-9639-4383-da54-e77478766394"
      },
      "outputs": [],
      "source": [
        "batch_data, batch_data_length, batch_event, batch_tte = next(iter(train_data_loader))\n",
        "\n",
        "for i in range(min(BATCH_SIZE, 16)):\n",
        "    print(\"batch event= %d --- batch_data_length= %d --- batch_tte= %d\" % (batch_event[i], batch_data_length[i], batch_tte[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBj68mNAYnpC"
      },
      "source": [
        "# 3. Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tojz4GxJYnpD"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "from deepHit import Encoder, CauseSpecificSubnetwork, DeepHit\n",
        "from baseline_losses import loss_1_batch, loss_2_batch\n",
        "\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "LEARNING_RATE_ENCODER = 1e-3\n",
        "LEARNING_RATE_CAUSESS = 1e-3\n",
        "\n",
        "LOSS_1_AMPLIFIER = 1\n",
        "LOSS_2_AMPLIFIER = 5\n",
        "\n",
        "RUN_VALIDATION_ROUND = True\n",
        "RUN_VALIDATION_ROUND_BATCHES_THRESHOLD = 2**2\n",
        "VAL_NUM_CASES_RUNTIME = BATCH_SIZE\n",
        "\n",
        "input_size = train_FREDDIEMAC_raw_dataset.get_num_covariates()\n",
        "output_size = train_FREDDIEMAC_raw_dataset.get_num_covariates()\n",
        "MAX_LENGTH = train_FREDDIEMAC_raw_dataset.get_max_length()\n",
        "\n",
        "NUM_CAUSES = 3\n",
        "hidden_size_encoder = 300\n",
        "context_size = 300\n",
        "hidden_cause_size = 50*NUM_CAUSES\n",
        "SIGMA = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVak9c7ZYnpD"
      },
      "source": [
        "# 4. Defining The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX-8nsBIYnpD"
      },
      "outputs": [],
      "source": [
        "# initialize model\n",
        "encoder = Encoder(input_size, hidden_size_encoder, context_size).to(DEVICE)\n",
        "causess = CauseSpecificSubnetwork(context_size, hidden_cause_size, input_size, MAX_LENGTH, NUM_CAUSES).to(DEVICE)\n",
        "DHT = DeepHit(encoder, causess, DEVICE)\n",
        "\n",
        "# intialize optimizer\n",
        "optimizer_encoder = Adam(encoder.parameters(), lr=LEARNING_RATE_ENCODER, weight_decay=1)\n",
        "optimizer_causess = Adam(causess.parameters(), lr=LEARNING_RATE_CAUSESS, weight_decay=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltuWGY_XYnpE"
      },
      "source": [
        "### 4.1 Testing a sample before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "lcTv4t9MYnpE",
        "outputId": "4ec47433-7998-4927-a5c7-1cf926837a14"
      },
      "outputs": [],
      "source": [
        "from utils import plot_fht_and_cif_baseline\n",
        "from baseline_losses import CIF_K\n",
        "\n",
        "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
        "\n",
        "test_batch_data = test_batch_data.to(DEVICE)\n",
        "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
        "test_batch_event = test_batch_event.to(DEVICE)\n",
        "\n",
        "DHT.eval()\n",
        "\n",
        "test_first_hitting_time = DHT(test_batch_data, test_batch_data_length)\n",
        "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
        "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
        "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
        "\n",
        "DHT.train()\n",
        "\n",
        "print(\"For the first sample of the test batch:\")\n",
        "print(\"sample has length %d\" % test_batch_data_length[0])\n",
        "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[0], model_tte_prediction[0] + 1))\n",
        "\n",
        "print(\"probability of prepay event = %.2f\" % CIF_K(test_first_hitting_time[0], 0, MAX_LENGTH)[-1].item())\n",
        "print(\"probability of default event = %.2f\" % CIF_K(test_first_hitting_time[0], 1, MAX_LENGTH)[-1].item())\n",
        "print(\"probability of full repay event = %.2f\" % CIF_K(test_first_hitting_time[0], 2, MAX_LENGTH)[-1].item())\n",
        "\n",
        "plot_fht_and_cif_baseline(test_first_hitting_time[0], MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gykwg0bSJ02O"
      },
      "source": [
        "# 5. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "b987dcdc7b0443418b2ccd71611fb6d4"
          ]
        },
        "id": "mK-9b74IYnpE",
        "outputId": "59887183-3099-4a88-a87f-55ee72b6a521"
      },
      "outputs": [],
      "source": [
        "#PATH = \"/content/gdrive/MyDrive/MP FEB/FREDDIEMAC/models/baseline_model_v5.pth\"\n",
        "\n",
        "# start training\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  train_epoch_val_loss = 0\n",
        "  train_epoch_val_loss1 = 0\n",
        "  train_epoch_val_loss2 = 0\n",
        "\n",
        "  for batch_number in range(len(train_data_loader)):\n",
        "    data = next(train_data_loader)\n",
        "\n",
        "    batch_loss = 0\n",
        "\n",
        "    optimizer_encoder.zero_grad()\n",
        "    optimizer_causess.zero_grad()\n",
        "\n",
        "    batch_data, batch_data_length, batch_event, batch_tte = data\n",
        "    batch_data = batch_data.to(DEVICE)\n",
        "    batch_data_length = batch_data_length.to(DEVICE)\n",
        "    batch_event = batch_event.to(DEVICE)\n",
        "    batch_tte = batch_tte.to(DEVICE)\n",
        "    \n",
        "    first_hitting_time_batch = DHT(batch_data, batch_data_length)\n",
        "\n",
        "    loss1 = LOSS_1_AMPLIFIER*loss_1_batch(first_hitting_time_batch, batch_event, batch_tte, MAX_LENGTH, DEVICE)\n",
        "    loss2 = LOSS_2_AMPLIFIER*loss_2_batch(first_hitting_time_batch, batch_event, batch_tte, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)\n",
        "\n",
        "    batch_loss = loss1 + loss2\n",
        "    batch_loss.backward()\n",
        "\n",
        "    epoch_loss += batch_loss.detach()\n",
        "\n",
        "    #wandb.log({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item()})\n",
        "    print({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item()})\n",
        "\n",
        "    optimizer_encoder.step()\n",
        "    optimizer_causess.step()\n",
        "\n",
        "    if RUN_VALIDATION_ROUND and batch_number % 2**4 == 0:\n",
        "      # validating round\n",
        "      #torch.save(DHT.state_dict(), PATH)\n",
        "      DHT.eval()\n",
        "\n",
        "      val_epoch_val_loss = 0\n",
        "      val_epoch_val_loss1 = 0\n",
        "      val_epoch_val_loss2 = 0\n",
        "\n",
        "      for validation_batch_number in range(len(validate_data_loader)):\n",
        "        data = next(validate_data_loader)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          val_batch_data, val_data_length, val_batch_event, val_batch_tte  = data\n",
        "          val_batch_data = val_batch_data.to(DEVICE)\n",
        "          val_data_length = val_data_length.to(DEVICE)\n",
        "          val_batch_event = val_batch_event.to(DEVICE)\n",
        "          val_batch_tte = val_batch_tte.to(DEVICE)\n",
        "\n",
        "          val_first_hitting_time_batch = DHT(val_batch_data, val_data_length)\n",
        "\n",
        "          val_loss1 = LOSS_1_AMPLIFIER*loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, MAX_LENGTH, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
        "          val_loss2 = LOSS_2_AMPLIFIER*loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
        "          \n",
        "          val_epoch_val_loss1 += val_loss1\n",
        "          val_epoch_val_loss2 += val_loss2\n",
        "          val_epoch_val_loss = val_loss1 + val_loss2\n",
        "\n",
        "      #wandb.log({\"val_epoch_val_loss1\": val_epoch_val_loss1.item(), \"val_epoch_val_loss2\": val_epoch_val_loss2.item(), \"val_epoch_val_loss\": val_epoch_val_loss.item()})\n",
        "      print({\"val_epoch_val_loss1\": val_epoch_val_loss1.item(), \"val_epoch_val_loss2\": val_epoch_val_loss2.item()})\n",
        "\n",
        "      DHT.train()\n",
        "      # end validating round\n",
        "\n",
        "  #wandb.log({\"train_epoch_loss\" : epoch_loss.item()})\n",
        "  #torch.save(DHT.state_dict(), PATH)\n",
        "\n",
        "#wandb.finish() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaFH9bp6J6js"
      },
      "source": [
        "# 6. Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2kFUiHWlbNDa",
        "outputId": "e3585183-50b4-42da-c0bc-e68067c79ee7"
      },
      "outputs": [],
      "source": [
        "#PATH = \"/content/gdrive/MyDrive/MP FEB/FREDDIEMAC/models/baseline_model_v5.pth\"\n",
        "#DHT.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXeTnuPZvqBf"
      },
      "source": [
        "### 6.1 Testing a sample after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gUsOb0O_YnpF",
        "outputId": "ea3b986e-e5fd-453f-8955-035be15547b2"
      },
      "outputs": [],
      "source": [
        "from utils import plot_fht_and_cif_baseline\n",
        "from baseline_losses import CIF_K\n",
        "\n",
        "test_sample_index = 0\n",
        "\n",
        "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
        "\n",
        "test_batch_data = test_batch_data.to(DEVICE)\n",
        "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
        "test_batch_event = test_batch_event.to(DEVICE)\n",
        "\n",
        "DHT.eval()\n",
        "\n",
        "test_first_hitting_time = DHT(test_batch_data, test_batch_data_length)\n",
        "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
        "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
        "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
        "\n",
        "print(\"For the first sample of the test batch:\")\n",
        "print(\"sample has length %d and will experience event %d\" % (test_batch_data_length[test_sample_index], test_batch_event[test_sample_index]))\n",
        "print(\"the model predicts at time %d the event %d \" % (model_tte_prediction[test_sample_index] + 1, model_event_prediction[test_sample_index]))\n",
        "\n",
        "print(\"probability of prepay event = %.2f\" % CIF_K(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH)[-1].item())\n",
        "print(\"probability of default event = %.2f\" % CIF_K(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH)[-1].item())\n",
        "print(\"probability of full repay event = %.2f\" % CIF_K(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH)[-1].item())\n",
        "\n",
        "plot_fht_and_cif_baseline(test_first_hitting_time[test_sample_index], MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f0C-WRCQNTQa",
        "outputId": "67651a70-d798-4421-aaa9-1279492d5dfe"
      },
      "outputs": [],
      "source": [
        "for test_sample_index in range(2**3):\n",
        "  print(\"sample will experiece event: %d - at time %d\" % (test_batch_event[test_sample_index], test_batch_data_length[test_sample_index]))\n",
        "  print(\"model will predict event:    %d - at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index]))\n",
        "\n",
        "  print(\"--------------------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bDK1ccduVKmV",
        "outputId": "f5ac14da-7023-4cc4-b370-b26d36172101"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "a = test_batch_event.flatten().cpu()\n",
        "b = model_event_prediction.cpu()\n",
        "\n",
        "cm = confusion_matrix(a, b)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['0', '1', '2', '3'])\n",
        "accuracy = cm.diagonal()/cm.sum(axis=1)\n",
        "\n",
        "print(accuracy)\n",
        "\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "xC80fhtPwLAi"
      ],
      "name": "baseline_FREDDIEMAC.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "18e97b579e8c0e40da8e2bba439fcd59aed88dbd21d3c026977017f366946867"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
