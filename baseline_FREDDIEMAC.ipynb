{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "# SRC_PATH = '/content/gdrive/MyDrive/MP FEB/Colab'\n",
    "# sys.path.append(SRC_PATH)\n",
    "\n",
    "# !pip install wandb -qqq\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "# wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"baseline_poc\", \n",
    "#     # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "#     #name=\"experiment 1\"\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     #config={\n",
    "#       #\"learning_rate\": 0.02,\n",
    "#       #\"architecture\": \"CNN\",\n",
    "#       #\"dataset\": \"CIFAR-100\",\n",
    "#       #\"epochs\": 10,}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = str(os.getcwd()) + \"\\data\"\n",
    "\n",
    "blumenstock_types = {'LOAN_SEQUENCE_NUMBER': str, 'INT_RATE': float, 'ORIG_UPB': float, 'FICO_SCORE': float,\n",
    "                    'DTI_R': float, 'LTV_R': float, 'FIRST_PAYMENT_DATE': str, 'BAL_REPAID': float, 'T_ACT_12M': float, 'T_DEL_30D': float, \n",
    "                    'T_DEL_60D': float, 'LABEL': float, 'REMAINING_MONTHS_TO_LEGAL_MATURITY': float, \"TIME_TO_EVENT\": float, 'TOTAL_OBSERVED_LENGTH': float}\n",
    "\n",
    "\n",
    "df_blumenstock = dd.read_parquet(data_folder + \"./blumenstock_labeled_sample_orig_*.parquet.gzip\")\n",
    "df_blumenstock = df_blumenstock.astype(blumenstock_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blumenstock.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Normalising raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_to_normalise = ['INT_RATE', 'ORIG_UPB', 'FICO_SCORE', 'DTI_R', 'LTV_R', 'REMAINING_MONTHS_TO_LEGAL_MATURITY',\n",
    "                           'BAL_REPAID', 'T_ACT_12M', 'T_DEL_30D', 'T_DEL_60D']\n",
    "\n",
    "df_blumenstoch_mean = df_blumenstock[covariates_to_normalise].mean().compute()\n",
    "df_blumenstoch_std = df_blumenstock[covariates_to_normalise].std().compute()\n",
    "\n",
    "df_blumenstock[covariates_to_normalise] = (df_blumenstock[covariates_to_normalise] - df_blumenstoch_mean) / df_blumenstoch_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blumenstock.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from FREDDIEMAC_baseline_data import FREDDIEMAC_basline_dataset, FREDDIEMAC_baseline_dataloader\n",
    "\n",
    "BATCH_SIZE = 2**8\n",
    "\n",
    "frac_cases=0.25\n",
    "test_set = False\n",
    "augment = False\n",
    "data_augment_factor = 3\n",
    "random_state = 42\n",
    "\n",
    "allowed_covariates = ['INT_RATE', 'ORIG_UPB', 'FICO_SCORE', 'DTI_R','LTV_R', 'BAL_REPAID', \n",
    "                     'T_ACT_12M', 'T_DEL_30D', 'T_DEL_60D', 'REMAINING_MONTHS_TO_LEGAL_MATURITY']\n",
    "\n",
    "TIME_TO_EVENT_covariate = 'TIME_TO_EVENT'\n",
    "LABEL_covariate = 'LABEL'\n",
    "\n",
    "\n",
    "FREDDIEMAC_raw_dataset = FREDDIEMAC_basline_dataset(df_blumenstock, \n",
    "                                                    allowed_covariates,\n",
    "                                                    TIME_TO_EVENT_covariate,\n",
    "                                                    LABEL_covariate,\n",
    "                                                    frac_cases,\n",
    "                                                    random_state,\n",
    "                                                    test_set,\n",
    "                                                    augment,\n",
    "                                                    data_augment_factor)\n",
    "\n",
    "print(\"This dataset will contain %d samples\" % len(FREDDIEMAC_raw_dataset))\n",
    "\n",
    "data_loader = FREDDIEMAC_dataloader(dataset=FREDDIEMAC_raw_dataset,\n",
    "                                    batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"This dataloader will deliver %d batches\" % data_loader.get_max_iterations())\n",
    "\n",
    "batch_data, batch_data_length, batch_event, batch_tte = next(data_loader)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)\n",
    "print(batch_tte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_data_length, batch_event, batch_tte = next(iter(data_loader))\n",
    "\n",
    "for i in range(min(BATCH_SIZE, 16)):\n",
    "    print(\"batch event= %d --- batch_data_length= %d --- batch_tte= %d\" % (batch_event[i], batch_data_length[i], batch_tte[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample (that is possibly in training set right now :p )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_cases=0.00004\n",
    "\n",
    "FREDDIEMAC_raw_dataset = FREDDIEMAC_basline_dataset(df_blumenstock, \n",
    "                                                    allowed_covariates,\n",
    "                                                    TIME_TO_EVENT_covariate,\n",
    "                                                    LABEL_covariate,\n",
    "                                                    frac_cases,\n",
    "                                                    random_state,\n",
    "                                                    test_set,\n",
    "                                                    augment,\n",
    "                                                    data_augment_factor)\n",
    "\n",
    "test_data_loader = FREDDIEMAC_dataloader(dataset=FREDDIEMAC_raw_dataset, batch_size=1)\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "print(\"This dataset will contain %d samples\" % len(FREDDIEMAC_raw_dataset))\n",
    "print(\"This dataloader will deliver %d batches\" % test_data_loader.get_max_iterations())\n",
    "print(\"batch event= %d --- batch_data_length= %d --- batch_tte= %d\" % (test_batch_event[0], test_batch_data_length[0], test_batch_tte[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from deepHit import Encoder, CauseSpecificSubnetwork, DeepHit\n",
    "from baseline_losses import loss_1_batch, loss_2_batch\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "LEARNING_RATE_ENCODER = 1e-3\n",
    "LEARNING_RATE_CAUSESS = 1e-3\n",
    "\n",
    "LOSS_1_AMPLIFIER = 1\n",
    "LOSS_2_AMPLIFIER = 1\n",
    "\n",
    "RUN_VALIDATION_ROUND = False\n",
    "VAL_NUM_CASES_RUNTIME = BATCH_SIZE\n",
    "\n",
    "input_size = FREDDIEMAC_raw_dataset.get_num_covariates()\n",
    "output_size = FREDDIEMAC_raw_dataset.get_num_covariates()\n",
    "MAX_LENGTH = FREDDIEMAC_raw_dataset.get_max_length()\n",
    "NUM_CAUSES = 3\n",
    "hidden_size_encoder = 512\n",
    "context_size = 256\n",
    "hidden_cause_size = 512\n",
    "SIGMA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "encoder = Encoder(input_size, hidden_size_encoder, context_size).to(DEVICE)\n",
    "causess = CauseSpecificSubnetwork(context_size, hidden_cause_size, input_size, MAX_LENGTH, NUM_CAUSES).to(DEVICE)\n",
    "DHT = DeepHit(encoder, causess, DEVICE)\n",
    "\n",
    "# intialize optimizer\n",
    "optimizer_encoder = Adam(encoder.parameters(), lr=LEARNING_RATE_ENCODER)\n",
    "optimizer_causess = Adam(causess.parameters(), lr=LEARNING_RATE_CAUSESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a sample before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_fht_and_cif_baseline\n",
    "from baseline_losses import CIF_K\n",
    "\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "test_batch_data = test_batch_data.unsqueeze(0).to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.unsqueeze(0).to(DEVICE)\n",
    "test_batch_event = test_batch_event.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "DHT.eval()\n",
    "\n",
    "test_first_hitting_time = DHT(test_batch_data, test_batch_data_length)\n",
    "print(\"sample has length %d\" % test_batch_data_length[0])\n",
    "\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax().item()\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction, model_tte_prediction + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K(test_first_hitting_time[0], 0, MAX_LENGTH)[23].item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K(test_first_hitting_time[0], 1, MAX_LENGTH)[23].item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K(test_first_hitting_time[0], 2, MAX_LENGTH)[23].item())\n",
    "\n",
    "plot_fht_and_cif_baseline(test_first_hitting_time[0], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"/content/gdrive/MyDrive/MP FEB/Colab/models/baseline_model_v4.pth\"\n",
    "\n",
    "train_data_loader = data_loader\n",
    "\n",
    "# start training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_number in range(len(train_data_loader)):\n",
    "    data = next(train_data_loader)\n",
    "\n",
    "    batch_loss = 0\n",
    "\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_causess.zero_grad()\n",
    "\n",
    "    batch_data, batch_data_length, batch_event, batch_tte = data\n",
    "    batch_data = batch_data.to(DEVICE)\n",
    "    batch_data_length = batch_data_length.to(DEVICE)\n",
    "    batch_event = batch_event.to(DEVICE)\n",
    "    batch_tte = batch_tte.to(DEVICE)\n",
    "    \n",
    "    first_hitting_time_batch = DHT(batch_data, batch_data_length)\n",
    "\n",
    "    loss1 = LOSS_1_AMPLIFIER*loss_1_batch(first_hitting_time_batch, batch_event, batch_tte, MAX_LENGTH, DEVICE)\n",
    "    loss2 = LOSS_2_AMPLIFIER*loss_2_batch(first_hitting_time_batch, batch_event, batch_tte, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)\n",
    "\n",
    "    batch_loss = loss1 + loss2\n",
    "    batch_loss.backward()\n",
    "\n",
    "    epoch_loss += batch_loss.detach()\n",
    "\n",
    "    #wandb.log({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item()})\n",
    "    print({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item()})\n",
    "\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_causess.step()\n",
    "\n",
    "    # if batch_number % 2**8 == 0:\n",
    "    #   torch.save(DHT.state_dict(), PATH)\n",
    "\n",
    "#   if RUN_VALIDATION_ROUND:\n",
    "#     # validating round\n",
    "#     DHT.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#       val_poc_raw_dataset = PocDataset(num_cases=VAL_NUM_CASES_RUNTIME)\n",
    "#       val_data_loader = torch.utils.data.DataLoader(val_poc_raw_dataset,batch_size=VAL_NUM_CASES_RUNTIME)\n",
    "#       val_batch_data, val_data_length, val_batch_event, val_batch_tte, _ = next(iter(val_data_loader))\n",
    "#       val_batch_data = val_batch_data.to(DEVICE)\n",
    "#       val_data_length = val_data_length.to(DEVICE)\n",
    "#       val_batch_event = val_batch_event.to(DEVICE)\n",
    "#       val_batch_tte = val_batch_tte.to(DEVICE)\n",
    "\n",
    "#       val_first_hitting_time_batch = DHT(val_batch_data, val_data_length)\n",
    "\n",
    "#       val_loss1 = LOSS_1_AMPLIFIER*loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, MAX_LENGTH, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "#       val_loss2 = LOSS_2_AMPLIFIER*loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "\n",
    "#       wandb.log({\"val_loss1\": val_loss1.item(), \"val_loss2\": val_loss2.item()})\n",
    "#       wandb.log({\"train_epoch_loss\" : epoch_loss.item(), \"val_epoch_loss\" : val_loss1.item() + val_loss2.item(),\"epoch\": epoch})\n",
    "\n",
    "#     DHT.train()\n",
    "#     # end validating round\n",
    "\n",
    "  #torch.save(DHT.state_dict(), PATH)\n",
    "\n",
    "#wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_fht_and_cif_baseline\n",
    "from baseline_losses import CIF_K\n",
    "\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "test_batch_data = test_batch_data.unsqueeze(0).to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.unsqueeze(0).to(DEVICE)\n",
    "test_batch_event = test_batch_event.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "DHT.eval()\n",
    "\n",
    "test_first_hitting_time = DHT(test_batch_data, test_batch_data_length)\n",
    "print(\"sample has length %d\" % test_batch_data_length[0])\n",
    "\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax().item()\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction, model_tte_prediction + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K(test_first_hitting_time[0], 0, MAX_LENGTH)[23].item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K(test_first_hitting_time[0], 1, MAX_LENGTH)[23].item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K(test_first_hitting_time[0], 2, MAX_LENGTH)[23].item())\n",
    "\n",
    "plot_fht_and_cif_baseline(test_first_hitting_time[0], MAX_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e97b579e8c0e40da8e2bba439fcd59aed88dbd21d3c026977017f366946867"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
