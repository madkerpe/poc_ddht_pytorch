{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "# SRC_PATH = '/content/gdrive/MyDrive/MP FEB/Colab'\n",
    "# sys.path.append(SRC_PATH)\n",
    "\n",
    "# !pip install wandb -qqq\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "# wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"baseline_poc\", \n",
    "#     # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "#     #name=\"experiment 1\"\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     #config={\n",
    "#       #\"learning_rate\": 0.02,\n",
    "#       #\"architecture\": \"CNN\",\n",
    "#       #\"dataset\": \"CIFAR-100\",\n",
    "#       #\"epochs\": 10,}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from poc_data import PocDataset, display_sample\n",
    "\n",
    "\n",
    "NUM_CASES = 2**9\n",
    "BATCH_SIZE = 2**8\n",
    "\n",
    "poc_raw_dataset = PocDataset(num_cases=NUM_CASES, augment=True)\n",
    "data_loader = DataLoader(dataset=poc_raw_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True)\n",
    "\n",
    "batch_data, batch_data_length, batch_event, batch_tte, _ = next(iter(data_loader))\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the longitudional data\n",
    "display_sample(batch_data[0], batch_data_length[0], batch_event[0])\n",
    "display_sample(batch_data[1], batch_data_length[1], batch_event[1])\n",
    "display_sample(batch_data[2], batch_data_length[2], batch_event[2])\n",
    "display_sample(batch_data[3], batch_data_length[3], batch_event[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poc_raw_dataset = PocDataset(num_cases=1)\n",
    "test_data_loader = DataLoader(dataset=test_poc_raw_dataset,batch_size=1,pin_memory=True)\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_tte, test_meta = next(iter(test_data_loader))\n",
    "display_sample(test_batch_data[0], test_batch_data_length[0], test_batch_event[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from deepHit import Encoder, CauseSpecificSubnetwork, DeepHit\n",
    "from baseline_losses import loss_1_batch, loss_2_batch\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "LEARNING_RATE_ENCODER = 0.001\n",
    "LEARNING_RATE_CAUSESS = 0.0005\n",
    "\n",
    "LOSS_1_AMPLIFIER = 1\n",
    "LOSS_2_AMPLIFIER = 1\n",
    "\n",
    "RUN_VALIDATION_ROUND = False\n",
    "VAL_NUM_CASES_RUNTIME = BATCH_SIZE\n",
    "\n",
    "input_size = 5\n",
    "output_size = input_size\n",
    "MAX_LENGTH = 2*12\n",
    "NUM_CAUSES = 3\n",
    "hidden_size_encoder = 512\n",
    "context_size = 256\n",
    "hidden_cause_size = 512\n",
    "SIGMA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "encoder = Encoder(input_size, hidden_size_encoder, context_size).to(DEVICE)\n",
    "causess = CauseSpecificSubnetwork(context_size, hidden_cause_size, input_size, MAX_LENGTH, NUM_CAUSES).to(DEVICE)\n",
    "DHT = DeepHit(encoder, causess, DEVICE)\n",
    "\n",
    "# intialize optimizer\n",
    "optimizer_encoder = Adam(encoder.parameters(), lr=LEARNING_RATE_ENCODER)\n",
    "optimizer_causess = Adam(causess.parameters(), lr=LEARNING_RATE_CAUSESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_number, data in enumerate(data_loader):\n",
    "\n",
    "    batch_loss = 0\n",
    "\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_causess.zero_grad()\n",
    "\n",
    "    batch_data, batch_data_length, batch_event, batch_tte, _ = data\n",
    "    batch_data = batch_data.to(DEVICE)\n",
    "    batch_data_length = batch_data_length.to(DEVICE)\n",
    "    batch_event = batch_event.to(DEVICE)\n",
    "    batch_tte = batch_tte.to(DEVICE)\n",
    "    \n",
    "    first_hitting_time_batch = DHT(batch_data, batch_data_length)\n",
    "\n",
    "    loss1 = LOSS_1_AMPLIFIER*loss_1_batch(first_hitting_time_batch, batch_event, batch_tte, MAX_LENGTH, DEVICE)\n",
    "    loss2 = LOSS_2_AMPLIFIER*loss_2_batch(first_hitting_time_batch, batch_event, batch_tte, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)\n",
    "\n",
    "    batch_loss = loss1 + loss2\n",
    "    batch_loss.backward()\n",
    "\n",
    "    epoch_loss += batch_loss.detach()\n",
    "\n",
    "    #wandb.log({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item()})\n",
    "\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_causess.step()\n",
    "\n",
    "  if RUN_VALIDATION_ROUND:\n",
    "    # validating round\n",
    "    DHT.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      val_poc_raw_dataset = PocDataset(num_cases=VAL_NUM_CASES_RUNTIME)\n",
    "      val_data_loader = torch.utils.data.DataLoader(val_poc_raw_dataset,batch_size=VAL_NUM_CASES_RUNTIME)\n",
    "      val_batch_data, val_data_length, val_batch_event, val_batch_tte, _ = next(iter(val_data_loader))\n",
    "      val_batch_data = val_batch_data.to(DEVICE)\n",
    "      val_data_length = val_data_length.to(DEVICE)\n",
    "      val_batch_event = val_batch_event.to(DEVICE)\n",
    "      val_batch_tte = val_batch_tte.to(DEVICE)\n",
    "\n",
    "      val_first_hitting_time_batch = DHT(val_batch_data, val_data_length)\n",
    "\n",
    "      val_loss1 = LOSS_1_AMPLIFIER*loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, MAX_LENGTH, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "      val_loss2 = LOSS_2_AMPLIFIER*loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "\n",
    "      #wandb.log({\"val_loss1\": val_loss1.item(), \"val_loss2\": val_loss2.item()})\n",
    "      ##wandb.log({\"train_epoch_loss\" : epoch_loss.item(), \"val_epoch_loss\" : val_loss1.item() + val_loss2.item(),\"epoch\": epoch})\n",
    "\n",
    "    DHT.train()\n",
    "    # end validating round\n",
    "\n",
    "  #PATH = \"/content/gdrive/MyDrive/MP FEB/Colab/models/baseline_model_v2.pth\"\n",
    "  #torch.save(DHT.state_dict(), PATH)\n",
    "\n",
    "#wandb.finish() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"C:/Users/marij/Desktop/THESIS ECO/DDHT/DDHT_pytorch/models/model_v11.pth\"\n",
    "#torch.save(DDHT.state_dict(), PATH)\n",
    "#DDHT.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poc_raw_dataset = PocDataset(num_cases=1)\n",
    "test_data_loader = DataLoader(dataset=test_poc_raw_dataset,batch_size=1,pin_memory=True)\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte, test_batch_meta = next(iter(test_data_loader))\n",
    "\n",
    "display_sample(test_batch_data[0], test_batch_data_length[0], test_batch_event[0])\n",
    "print(test_batch_data[0,:5,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_fht_and_cif_baseline\n",
    "\n",
    "DHT.eval();\n",
    "test_first_hitting_time = DHT(test_batch_data.to(DEVICE), test_batch_data_length.to(DEVICE))\n",
    "\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax().item()\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "print(\"the model predicts at time %d the event %d\" % (model_tte_prediction, model_event_prediction))\n",
    "\n",
    "plot_fht_and_cif_baseline(test_first_hitting_time[0], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring our model - random test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_NUM_CASES = 2**6\n",
    "\n",
    "val_poc_raw_dataset = PocDataset(num_cases=VAL_NUM_CASES)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_poc_raw_dataset,batch_size=2**6)\n",
    "val_batch_data, val_batch_data_length, val_batch_event, val_batch_tte, val_batch_meta = next(iter(val_data_loader))\n",
    "val_batch_data = val_batch_data.to(DEVICE)\n",
    "val_batch_data_length = val_batch_data_length.to(DEVICE)\n",
    "val_batch_event = val_batch_event.to(DEVICE)\n",
    "val_batch_tte = val_batch_tte.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DHT.eval()\n",
    "\n",
    "val_first_hitting_time_batch = DHT(val_batch_data, val_batch_data_length)\n",
    "val_loss1 = LOSS_1_AMPLIFIER*loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, MAX_LENGTH, DEVICE)/VAL_NUM_CASES\n",
    "val_loss2 = LOSS_2_AMPLIFIER*loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES\n",
    "\n",
    "\n",
    "print(\"val_loss1=\", val_loss1.item())\n",
    "print(\"val_loss2=\", val_loss2.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring our model - fixed test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_fht_and_cif\n",
    "\n",
    "test_poc_raw_dataset = PocDataset(num_cases=1)\n",
    "test_data_loader = poc_raw_dataset = PocDataset(num_cases=1, test_set=True, repays=False)\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte, test_meta = next(iter(test_data_loader))\n",
    "test_batch_data = test_batch_data.unsqueeze(0).to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.unsqueeze(0).to(DEVICE)\n",
    "test_batch_event = test_batch_event.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "DHT.eval()\n",
    "\n",
    "test_first_hitting_time = DHT(test_batch_data, test_batch_data_length)\n",
    "print(\"sample has length %d\" % test_batch_data_length[0])\n",
    "print(\"sample will experience event %d at time %d, but shows event %d\" % (test_meta['ground_truth_event'], test_meta['age'], test_batch_event[0]))\n",
    "\n",
    "\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax().item()\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction, model_tte_prediction + 1))\n",
    "\n",
    "plot_fht_and_cif_baseline(test_first_hitting_time[0], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"/content/gdrive/MyDrive/MP FEB/Colab/models/model_v10.pth\"\n",
    "# torch.save(DDHT.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e97b579e8c0e40da8e2bba439fcd59aed88dbd21d3c026977017f366946867"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
