{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLzSaW7cNu9q"
      },
      "source": [
        "# 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "COLAB = True\n",
        "MODEL_VERSION = 3"
      ],
      "metadata": {
        "id": "G8fLsplTRsbr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Cu1vkoIZNu9v",
        "outputId": "83f9b387-5f80-49fc-e2a7-5dbb766fe0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmadkerpe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220810_200119-1mxd1hr8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/madkerpe/main_FREDDIEMAC/runs/1mxd1hr8\" target=\"_blank\">earthy-voice-10</a></strong> to <a href=\"https://wandb.ai/madkerpe/main_FREDDIEMAC\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if COLAB:\n",
        "  import sys\n",
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount('/content/gdrive', force_remount=True)\n",
        "  SRC_PATH = '/content/gdrive/MyDrive/MP FEB/FREDDIEMAC'\n",
        "  DATA_SRC_PATH = SRC_PATH + \"/data\"\n",
        "  sys.path.append(SRC_PATH)\n",
        "\n",
        "  !pip install wandb -qqq\n",
        "  import wandb\n",
        "  wandb.login()\n",
        "\n",
        "  !pip install dask[dataframe] -qqq\n",
        "  !pip install fastparquet python-snappy -qqq\n",
        "  import dask.dataframe as dd\n",
        "\n",
        "\n",
        "\n",
        "  wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"main_FREDDIEMAC\", \n",
        "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      #name=\"experiment 1\"\n",
        "      # Track hyperparameters and run metadata\n",
        "      #config={\n",
        "        #\"learning_rate\": 0.02,\n",
        "        #\"architecture\": \"CNN\",\n",
        "        #\"dataset\": \"CIFAR-100\",\n",
        "        #\"epochs\": 10,}\n",
        "      )\n",
        "else:\n",
        "  DATA_SRC_PATH = \"./data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aYmp-1KtNu9y"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "DEVICE = 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4glL2wSNu9z"
      },
      "source": [
        "# 2. Import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGorKD_RNu9z"
      },
      "source": [
        "### 2.1 Loading raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_v8kRRWNu90",
        "outputId": "cbbc87ef-0f45-439d-fca3-d149b306fdf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataframe contains a total of 5630970 entries\n",
            "This dataframe contains a total of 83513 loands\n"
          ]
        }
      ],
      "source": [
        "blumenstock_dynamic_types = {'LOAN_SEQUENCE_NUMBER': str, 'MONTHLY_REPORTING_PERIOD': str,'CURRENT_ACTUAL_UPB': float, 'CURRENT_LOAN_DELINQUENCY_STATUS': float, \n",
        "                    'CURRENT_INTEREST_RATE':float,'ELTV': float ,'LOAN_AGE': float, 'REMAINING_MONTHS_TO_LEGAL_MATURITY': float, 'CREDIT_SCORE': float,\n",
        "                    'DTI': float, 'LTV': float, 'BAL_REPAID': float, \n",
        "                    'LABEL': float, \"TIME_TO_EVENT\": float, 'ORIGINAL_INTEREST_RATE': float, 'ORIGINAL_UPB': float, 'TOTAL_OBSERVED_LENGTH': float}\n",
        "\n",
        "df_blumenstock_dynamic = dd.read_parquet(DATA_SRC_PATH + \"/blumenstock_dynamic_labeled_sample_orig_*.parquet.gzip\")\n",
        "df_blumenstock_dynamic = df_blumenstock_dynamic.astype(blumenstock_dynamic_types)\n",
        "\n",
        "print(\"This dataframe contains a total of %d entries\" % len(df_blumenstock_dynamic))\n",
        "print(\"This dataframe contains a total of %d loands\" % len(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jlgpz-XNu91"
      },
      "source": [
        "### 2.2 Normalising raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gGTe6J86Nu92"
      },
      "outputs": [],
      "source": [
        "covariates_to_normalise = ['CURRENT_ACTUAL_UPB', 'CURRENT_LOAN_DELINQUENCY_STATUS', 'CURRENT_INTEREST_RATE', 'ELTV', 'LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY', 'CREDIT_SCORE',\n",
        "                            'DTI', 'LTV', 'BAL_REPAID', 'ORIGINAL_INTEREST_RATE', 'ORIGINAL_UPB']\n",
        "\n",
        "df_blumenstoch_dynamic_mean = df_blumenstock_dynamic[covariates_to_normalise].mean().compute()\n",
        "df_blumenstoch_std = df_blumenstock_dynamic[covariates_to_normalise].std().compute()\n",
        "\n",
        "df_blumenstock_dynamic[covariates_to_normalise] = (df_blumenstock_dynamic[covariates_to_normalise] - df_blumenstoch_dynamic_mean) / df_blumenstoch_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pD0jp-gNu93"
      },
      "source": [
        "### 2.3 Splitting train, validation and test-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFxeqnD5Nu93",
        "outputId": "09c66196-220b-4bb5-fc8d-d258e7c5f4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train set contains 1106465 entries\n",
            "Total validation set contains 34611 entries\n",
            "Total test set contains 17490 entries\n",
            "------------------------------------------------------------------\n",
            "Total train set contains 16385 loans\n",
            "Total validation set contains 513 loans\n",
            "Total test set contains 257 loans\n"
          ]
        }
      ],
      "source": [
        "#TODO THESE SETS ARE OVERLAPPING!!!\n",
        "\n",
        "AMOUNT_OF_TRAIN_LOANS = 2**14 + 1\n",
        "AMOUNT_OF_VALIDATE_LOANS = 2**9 + 1\n",
        "AMOUNT_OF_TEST_LOANS = 2**8 + 1\n",
        "\n",
        "train_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_TRAIN_LOANS, replace=False)\n",
        "validate_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_VALIDATE_LOANS, replace=False)\n",
        "test_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_TEST_LOANS, replace=False)\n",
        "\n",
        "validate_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(validate_choices)]\n",
        "train_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(train_choices)]\n",
        "test_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(test_choices)]\n",
        "\n",
        "print(\"Total train set contains %d entries\" % len(train_df_blumenstock))\n",
        "print(\"Total validation set contains %d entries\" % len(validate_df_blumenstock))\n",
        "print(\"Total test set contains %d entries\" % len(test_df_blumenstock))\n",
        "\n",
        "print(\"------------------------------------------------------------------\")\n",
        "print(\"Total train set contains %d loans\" % len(train_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))\n",
        "print(\"Total validation set contains %d loans\" % len(validate_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))\n",
        "print(\"Total test set contains %d loans\" % len(test_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TpM9FTwNu94"
      },
      "source": [
        "### 2.4 Creating dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX1MuJ5iNu95",
        "outputId": "c0243742-cd31-4ad3-bc08-df75f1ea6713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size =  256\n",
            "number of covariates =  12\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from FREDDIEMAC_main_data import FREDDIEMAC_main_dataset, FREDDIEMAC_main_dataloader\n",
        "\n",
        "BATCH_SIZE = 2**8\n",
        "\n",
        "allowed_covariates = ['CURRENT_ACTUAL_UPB', 'CURRENT_LOAN_DELINQUENCY_STATUS', 'CURRENT_INTEREST_RATE', 'ELTV', \n",
        "                             'LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY', 'CREDIT_SCORE', 'DTI', 'LTV', 'BAL_REPAID', \n",
        "                             'ORIGINAL_INTEREST_RATE', 'ORIGINAL_UPB']\n",
        "\n",
        "TOTAL_OBSERVED_LENGTH_covariate = 'TOTAL_OBSERVED_LENGTH'\n",
        "TIME_TO_EVENT_covariate ='TIME_TO_EVENT'\n",
        "LABEL_covariate = 'LABEL'\n",
        "\n",
        "random_state = 123\n",
        "augment = False\n",
        "data_augment_factor = 3\n",
        "\n",
        "print(\"batch_size = \", BATCH_SIZE)\n",
        "print(\"number of covariates = \", len(allowed_covariates))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lw5qlJeNu95"
      },
      "source": [
        "#### 2.4.1 Creating train dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzm2JvlGNu95",
        "outputId": "f4c2639d-9f8d-4376-be87-6cd085d8950e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset will contain 65540 samples\n",
            "This dataloader will deliver 256 batches\n",
            "torch.Size([256, 180, 12])\n",
            "torch.Size([256, 1])\n",
            "torch.Size([256, 1])\n",
            "torch.Size([256, 1])\n"
          ]
        }
      ],
      "source": [
        "augment_train_data = True\n",
        "\n",
        "train_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(train_df_blumenstock, \n",
        "                                                        allowed_covariates,\n",
        "                                                        TIME_TO_EVENT_covariate,\n",
        "                                                        TOTAL_OBSERVED_LENGTH_covariate,\n",
        "                                                        LABEL_covariate,\n",
        "                                                        frac_cases=1,\n",
        "                                                        random_state=random_state,\n",
        "                                                        test_set=False,\n",
        "                                                        augment=augment_train_data,\n",
        "                                                        data_augment_factor=data_augment_factor)\n",
        "\n",
        "print(\"This dataset will contain %d samples\" % len(train_FREDDIEMAC_raw_dataset))\n",
        "train_data_loader = FREDDIEMAC_main_dataloader(dataset=train_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
        "print(\"This dataloader will deliver %d batches\" % train_data_loader.get_max_iterations())\n",
        "batch_data, batch_data_length, batch_event, batch_tte = next(train_data_loader)\n",
        "\n",
        "print(batch_data.shape)\n",
        "print(batch_data_length.shape)\n",
        "print(batch_event.shape)\n",
        "print(batch_tte.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "088I-oe6Nu96"
      },
      "source": [
        "#### 2.4.2 Creating validate dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyuCc-a7Nu96",
        "outputId": "d61eb9a2-ec6c-4d0d-b2c9-5a75f7bfe0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset will contain 513 samples\n",
            "This dataloader will deliver 2 batches\n",
            "torch.Size([256, 180, 12])\n",
            "torch.Size([256, 1])\n",
            "torch.Size([256, 1])\n",
            "torch.Size([256, 1])\n"
          ]
        }
      ],
      "source": [
        "augment_validate_data = False\n",
        "\n",
        "validate_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(validate_df_blumenstock, \n",
        "                                                            allowed_covariates,\n",
        "                                                            TIME_TO_EVENT_covariate,\n",
        "                                                            TOTAL_OBSERVED_LENGTH_covariate,\n",
        "                                                            LABEL_covariate,\n",
        "                                                            frac_cases=1,\n",
        "                                                            random_state=random_state,\n",
        "                                                            test_set=False,\n",
        "                                                            augment=augment_validate_data,\n",
        "                                                            data_augment_factor=data_augment_factor)\n",
        "\n",
        "print(\"This dataset will contain %d samples\" % len(validate_FREDDIEMAC_raw_dataset))\n",
        "validate_data_loader = FREDDIEMAC_main_dataloader(dataset=validate_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
        "print(\"This dataloader will deliver %d batches\" % validate_data_loader.get_max_iterations())\n",
        "batch_data, batch_data_length, batch_event, batch_tte = next(validate_data_loader)\n",
        "\n",
        "print(batch_data.shape)\n",
        "print(batch_data_length.shape)\n",
        "print(batch_event.shape)\n",
        "print(batch_tte.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6obhVVl7Nu96"
      },
      "source": [
        "#### 2.4.3 Creating a test sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptCSgQ2hNu97",
        "outputId": "612c1746-a626-403e-a4da-808c709344b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset will contain 257 samples\n",
            "This dataloader will deliver 1 batches\n",
            "torch.Size([256, 180, 12])\n",
            "torch.Size([256, 1])\n",
            "torch.Size([256, 1])\n",
            "torch.Size([256, 1])\n"
          ]
        }
      ],
      "source": [
        "augment_test_data = False\n",
        "\n",
        "test_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(test_df_blumenstock, \n",
        "                                                    allowed_covariates,\n",
        "                                                    TIME_TO_EVENT_covariate,\n",
        "                                                    TOTAL_OBSERVED_LENGTH_covariate,\n",
        "                                                    LABEL_covariate,\n",
        "                                                    frac_cases=1,\n",
        "                                                    random_state=random_state,\n",
        "                                                    test_set=False,\n",
        "                                                    augment=augment_test_data,\n",
        "                                                    data_augment_factor=data_augment_factor)\n",
        "\n",
        "print(\"This dataset will contain %d samples\" % len(test_FREDDIEMAC_raw_dataset))\n",
        "test_data_loader = FREDDIEMAC_main_dataloader(dataset=test_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
        "print(\"This dataloader will deliver %d batches\" % test_data_loader.get_max_iterations())\n",
        "batch_data, batch_data_length, batch_event, batch_tte = next(test_data_loader)\n",
        "\n",
        "print(batch_data.shape)\n",
        "print(batch_data_length.shape)\n",
        "print(batch_event.shape)\n",
        "print(batch_tte.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drbUAQY_Nu97"
      },
      "source": [
        "### 2.5 Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju_bHmLkNu97",
        "outputId": "83b4352a-8866-4739-c1ed-ee2599af6642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch event= 1 --- batch_data_length= 38 --- batch_tte= 40\n",
            "batch event= 3 --- batch_data_length= 90 --- batch_tte= 90\n",
            "batch event= 3 --- batch_data_length= 83 --- batch_tte= 83\n",
            "batch event= 0 --- batch_data_length= 48 --- batch_tte= 72\n",
            "batch event= 3 --- batch_data_length= 120 --- batch_tte= 120\n",
            "batch event= 0 --- batch_data_length= 87 --- batch_tte= 120\n",
            "batch event= 3 --- batch_data_length= 53 --- batch_tte= 53\n",
            "batch event= 0 --- batch_data_length= 20 --- batch_tte= 20\n",
            "batch event= 0 --- batch_data_length= 56 --- batch_tte= 56\n",
            "batch event= 0 --- batch_data_length= 77 --- batch_tte= 105\n",
            "batch event= 3 --- batch_data_length= 120 --- batch_tte= 120\n",
            "batch event= 0 --- batch_data_length= 20 --- batch_tte= 20\n",
            "batch event= 0 --- batch_data_length= 20 --- batch_tte= 29\n",
            "batch event= 3 --- batch_data_length= 73 --- batch_tte= 73\n",
            "batch event= 3 --- batch_data_length= 95 --- batch_tte= 95\n",
            "batch event= 0 --- batch_data_length= 100 --- batch_tte= 100\n"
          ]
        }
      ],
      "source": [
        "batch_data, batch_data_length, batch_event, batch_tte = next(iter(train_data_loader))\n",
        "\n",
        "for i in range(min(BATCH_SIZE, 16)):\n",
        "    print(\"batch event= %d --- batch_data_length= %d --- batch_tte= %d\" % (batch_event[i], batch_data_length[i], batch_tte[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-az9i8pNu98"
      },
      "source": [
        "# 3. Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jlZiVMZLNu98"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "from dynamicDeepHit import EncoderRNN, AttnDecoderRNN, CauseSpecificSubnetwork, DynamicDeepHit\n",
        "from losses import loss_1_batch, loss_2_batch, loss_3_batch\n",
        "\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "LEARNING_RATE_ENCODER = 0.001\n",
        "LEARNING_RATE_DECODER = 0.001\n",
        "LEARNING_RATE_CAUSESS = 0.0005\n",
        "\n",
        "LOSS_1_AMPLIFIER = 1\n",
        "LOSS_2_AMPLIFIER = 4\n",
        "LOSS_3_AMPLIFIER = 10\n",
        "\n",
        "RUN_VALIDATION_ROUND = True\n",
        "RUN_VALIDATION_ROUND_BATCHES_THRESHOLD = 2**4\n",
        "VAL_NUM_CASES_RUNTIME = BATCH_SIZE\n",
        "\n",
        "input_size = train_FREDDIEMAC_raw_dataset.get_num_covariates()\n",
        "output_size = input_size\n",
        "MAX_LENGTH = train_FREDDIEMAC_raw_dataset.get_max_length()\n",
        "\n",
        "NUM_CAUSES = 3\n",
        "SIGMA = 0.1\n",
        "\n",
        "rnn_state_size = 10\n",
        "\n",
        "encoder_fc_size = 300\n",
        "attention_fc_size = 300\n",
        "cause_fc_size = 50*NUM_CAUSES\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLhFpHwvNu98"
      },
      "source": [
        "# 4. Defining The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IosIj_a6Nu99"
      },
      "outputs": [],
      "source": [
        "# initialize model\n",
        "encoder = EncoderRNN(input_size, rnn_state_size, encoder_fc_size, output_size).to(DEVICE)\n",
        "decoder = AttnDecoderRNN(rnn_state_size, input_size, attention_fc_size).to(DEVICE)\n",
        "causess = CauseSpecificSubnetwork(rnn_state_size, input_size, cause_fc_size, MAX_LENGTH, NUM_CAUSES).to(DEVICE)\n",
        "DDHT = DynamicDeepHit(encoder, decoder, causess, MAX_LENGTH, DEVICE)\n",
        "\n",
        "# intialize optimizer\n",
        "optimizer_encoder = Adam(encoder.parameters(), lr=LEARNING_RATE_ENCODER, weight_decay=1)\n",
        "optimizer_decoder = Adam(decoder.parameters(), lr=LEARNING_RATE_DECODER, weight_decay=1)\n",
        "optimizer_causess = Adam(causess.parameters(), lr=LEARNING_RATE_CAUSESS, weight_decay=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuDtFo1HNu99"
      },
      "source": [
        "### 4.1 Testing a sample before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "j1f7jtjkNu99",
        "outputId": "bd5e8b38-8ac5-4829-fad2-e988978e0887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the first sample of the test batch:\n",
            "sample has length 36\n",
            "the model predicts the event 1 at time 42\n",
            "probability of prepay event = 0.34\n",
            "probability of default event = 0.33\n",
            "probability of full repay event = 0.33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from utils import plot_fht_and_cif, plot_fht, plot_cif\n",
        "from losses import CIF_K_tau\n",
        "\n",
        "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
        "\n",
        "test_batch_data = test_batch_data.to(DEVICE)\n",
        "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
        "test_batch_event = test_batch_event.to(DEVICE)\n",
        "\n",
        "DDHT.eval()\n",
        "\n",
        "test_output, test_first_hitting_time, _ = DDHT(test_batch_data, test_batch_data_length)\n",
        "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
        "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
        "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
        "\n",
        "DDHT.train()\n",
        "\n",
        "print(\"For the first sample of the test batch:\")\n",
        "print(\"sample has length %d\" % test_batch_data_length[0])\n",
        "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[0], model_tte_prediction[0] + 1))\n",
        "\n",
        "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 0, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
        "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 1, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
        "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 2, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
        "\n",
        "plot_fht_and_cif(test_first_hitting_time[0], test_batch_data_length[0], MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ2j809TNu99"
      },
      "source": [
        "# 5. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_PATH(version, epoch_number=None, batch_number=None, colab=True):\n",
        "  \n",
        "  if epoch_number is None:\n",
        "    epoch_number = ''\n",
        "\n",
        "  if batch_number is None:\n",
        "    batch_number = ''\n",
        "  \n",
        "  if colab:\n",
        "    return f\"/content/gdrive/MyDrive/MP FEB/FREDDIEMAC/models/main_model_e{epoch_number}_b{batch_number}_v{version}.pth\"\n",
        "  else:\n",
        "    return f\"models/main_model_local_e{epoch_number}_b{batch_number}_v{version}.pth\""
      ],
      "metadata": {
        "id": "d2lKYplXPQku"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8yRw2wONu9-"
      },
      "outputs": [],
      "source": [
        "# start training\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "  epoch_loss = 0\n",
        "  train_epoch_val_loss = 0\n",
        "  train_epoch_val_loss1 = 0\n",
        "  train_epoch_val_loss2 = 0\n",
        "\n",
        "  for batch_number in range(len(train_data_loader)):\n",
        "    data = next(train_data_loader)\n",
        "\n",
        "    batch_loss = 0\n",
        "\n",
        "    optimizer_encoder.zero_grad()\n",
        "    optimizer_decoder.zero_grad()\n",
        "    optimizer_causess.zero_grad()\n",
        "\n",
        "    batch_data, batch_data_length, batch_event, batch_tte = data\n",
        "    batch_data = batch_data.to(DEVICE)\n",
        "    batch_data_length = batch_data_length.to(DEVICE)\n",
        "    batch_event = batch_event.to(DEVICE)\n",
        "    batch_tte = batch_tte.to(DEVICE)\n",
        "    \n",
        "    output_batch, first_hitting_time_batch, _ = DDHT(batch_data, batch_data_length)\n",
        "\n",
        "    loss1 = LOSS_1_AMPLIFIER*loss_1_batch(first_hitting_time_batch, batch_event, batch_tte, batch_data_length, MAX_LENGTH, DEVICE)\n",
        "    loss2 = LOSS_2_AMPLIFIER*loss_2_batch(first_hitting_time_batch, batch_event, batch_tte, batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)\n",
        "    loss3 = LOSS_3_AMPLIFIER*loss_3_batch(output_batch, batch_data.detach())\n",
        "\n",
        "    batch_loss = loss1 + loss2 + loss3\n",
        "    batch_loss.backward()\n",
        "\n",
        "    epoch_loss += batch_loss.detach()\n",
        "\n",
        "    if COLAB:\n",
        "      wandb.log({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item(), \"train_loss3\": loss3.item()})\n",
        "    else:\n",
        "      print({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item(), \"train_loss3\": loss3.item()})\n",
        "\n",
        "    optimizer_encoder.step()\n",
        "    optimizer_decoder.step()\n",
        "    optimizer_causess.step()\n",
        "\n",
        "    if RUN_VALIDATION_ROUND and batch_number % 5 == 0:\n",
        "      # validation round\n",
        "      if COLAB:\n",
        "        torch.save(DDHT.state_dict(), get_PATH(MODEL_VERSION, epoch, batch_number, COLAB))\n",
        "      DDHT.eval()\n",
        "\n",
        "      val_epoch_val_loss = 0\n",
        "      val_epoch_val_loss1 = 0\n",
        "      val_epoch_val_loss2 = 0\n",
        "      val_epoch_val_loss3 = 0\n",
        "\n",
        "      VAL_NUM_CASES_RUNTIME = len(validate_data_loader)*BATCH_SIZE\n",
        "      for validation_batch_number in range(len(validate_data_loader)):\n",
        "        data = next(validate_data_loader)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          val_batch_data, val_batch_data_length, val_batch_event, val_batch_tte = next(iter(validate_data_loader))\n",
        "          val_batch_data = val_batch_data.to(DEVICE)\n",
        "          val_batch_data_length = val_batch_data_length.to(DEVICE)\n",
        "          val_batch_event = val_batch_event.to(DEVICE)\n",
        "          val_batch_tte = val_batch_tte.to(DEVICE)\n",
        "\n",
        "          val_output_batch, val_first_hitting_time_batch, _ = DDHT(val_batch_data, val_batch_data_length)\n",
        "\n",
        "          val_loss1 = LOSS_1_AMPLIFIER*loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, val_batch_data_length, MAX_LENGTH, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
        "          val_loss2 = LOSS_2_AMPLIFIER*loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, val_batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
        "          val_loss3 = LOSS_3_AMPLIFIER*loss_3_batch(val_output_batch, val_batch_data.detach())/VAL_NUM_CASES_RUNTIME\n",
        "\n",
        "          val_epoch_val_loss1 += val_loss1\n",
        "          val_epoch_val_loss2 += val_loss2\n",
        "          val_epoch_val_loss3 += val_loss3\n",
        "          val_epoch_val_loss = val_loss1 + val_loss2 + val_loss3\n",
        "\n",
        "      if COLAB:\n",
        "        wandb.log({\"val_epoch_val_loss1\": val_epoch_val_loss1.item(), \"val_epoch_val_loss2\": val_epoch_val_loss2.item(), \"val_epoch_val_loss3\": val_epoch_val_loss3.item(), \"val_epoch_val_loss\": val_epoch_val_loss.item()})\n",
        "      else:\n",
        "        print({\"val_epoch_val_loss1\": val_epoch_val_loss1.item(), \"val_epoch_val_loss2\": val_epoch_val_loss2.item(), \"val_epoch_val_loss3\": val_epoch_val_loss3.item()})\n",
        "\n",
        "      DDHT.train()\n",
        "      # end validating round\n",
        "\n",
        "  if COLAB:\n",
        "    wandb.log({\"train_epoch_loss\": epoch_loss.item()})\n",
        "  torch.save(DDHT.state_dict(), get_PATH(MODEL_VERSION, None, None, COLAB))\n",
        "\n",
        "if COLAB:\n",
        "  wandb.finish() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0FQt3aJNu9-"
      },
      "source": [
        "# 6. Testing the models performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdzzvaWgNu9-"
      },
      "outputs": [],
      "source": [
        "LOCAL_PATH = \"models/main_model_v2_464.pth\"\n",
        "\n",
        "if COLAB:\n",
        "  DDHT.load_state_dict(torch.load(get_PATH(MODEL_VERSION, None, None, COLAB)))\n",
        "else:\n",
        "  DDHT.load_state_dict(torch.load(LOCAL_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7hIwR4JNu9_"
      },
      "outputs": [],
      "source": [
        "test_sample_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6-S7WNsNu9_"
      },
      "source": [
        "### 6.1 Testing a sample after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFW2XwJoNu9_"
      },
      "outputs": [],
      "source": [
        "from utils import plot_fht_and_cif, plot_fht, plot_cif\n",
        "from losses import CIF_K_tau\n",
        "\n",
        "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
        "\n",
        "test_batch_data = test_batch_data.to(DEVICE)\n",
        "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
        "test_batch_event = test_batch_event.to(DEVICE)\n",
        "\n",
        "DDHT.eval()\n",
        "\n",
        "test_output, test_first_hitting_time, test_attention_weights = DDHT(test_batch_data, test_batch_data_length)\n",
        "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
        "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
        "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
        "\n",
        "print(\"For the first sample of the test batch:\")\n",
        "print(\"sample has length %d\" % test_batch_data_length[test_sample_index])\n",
        "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
        "\n",
        "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "\n",
        "plot_fht_and_cif(test_first_hitting_time[test_sample_index], test_batch_data_length[test_sample_index], MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9KU5fM5Nu9_"
      },
      "source": [
        "### 6.2 Testing more samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDWM4I7bNu-A"
      },
      "outputs": [],
      "source": [
        "for test_sample_index in range(2**3):\n",
        "  print(\"--------------------------------------------\")\n",
        "  print(\"sample will experiece event: %d - at time %d\" % (test_batch_event[test_sample_index], test_batch_data_length[test_sample_index]))\n",
        "  print(\"model will predict event:    %d - at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqJcUNQSNu-A"
      },
      "source": [
        "### 6.3 Plotting a confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7sjhzkHNu-A"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "a = test_batch_event.flatten().cpu()\n",
        "b = model_event_prediction.cpu()\n",
        "\n",
        "cm = confusion_matrix(a, b)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['prepay', 'default', 'repay'])\n",
        "accuracy = cm.diagonal()/cm.sum(axis=1)\n",
        "\n",
        "print(accuracy)\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWu8Vm1ENu-A"
      },
      "source": [
        "# 7. Analysing the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fnS5xu8Nu-A"
      },
      "source": [
        "### 7.1 Which attention weights are the most important?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWERaX9mNu-B"
      },
      "outputs": [],
      "source": [
        "from utils import plot_attention_weights\n",
        "plot_attention_weights(test_attention_weights[test_sample_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gXoIuxYNu-B"
      },
      "outputs": [],
      "source": [
        "from utils import plot_attention_weights_batch\n",
        "\n",
        "plot_attention_weights_batch(test_attention_weights, test_batch_data_length, normalised=True, y_lim=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ98QZ_ZNu-B"
      },
      "source": [
        "### 7.2 What does the model do when we mask some covariates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ3BTybGNu-B"
      },
      "outputs": [],
      "source": [
        "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
        "\n",
        "test_batch_data = test_batch_data.to(DEVICE)\n",
        "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
        "test_batch_event = test_batch_event.to(DEVICE)\n",
        "\n",
        "masked_data = copy.deepcopy(test_batch_data)\n",
        "masked_covariates = ['LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY']\n",
        "\n",
        "for covariate in masked_covariates:\n",
        "  masked_data[:,:,allowed_covariates.index(covariate)] = 0.0\n",
        "\n",
        "masked_data = masked_data.to(DEVICE)\n",
        "\n",
        "test_output, test_first_hitting_time, test_attention_weights = DDHT(masked_data, test_batch_data_length)\n",
        "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
        "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
        "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
        "\n",
        "\n",
        "print(\"from the allowed covariates: \", allowed_covariates)\n",
        "print(\"we masked the following covariates: \", masked_covariates)\n",
        "\n",
        "print(\"For the first sample of the test batch:\")\n",
        "print(\"sample has length %d\" % test_batch_data_length[test_sample_index])\n",
        "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
        "\n",
        "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "\n",
        "plot_fht_and_cif(test_first_hitting_time[test_sample_index], test_batch_data_length[test_sample_index], MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jWcuCEMNu-B"
      },
      "source": [
        "### 7.3 Probability that this loan will prepay, default or repay in the comming \"delta\" months?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIRG3eOnNu-B"
      },
      "outputs": [],
      "source": [
        "delta = 6  #This is variable\n",
        "\n",
        "evaluation_time = min(int(test_batch_data_length[test_sample_index].item()) + delta, MAX_LENGTH)\n",
        "print(\"In the comming %d months, the probability that a specific event happens is:\" % delta)\n",
        "\n",
        "p_ev0 = CIF_K_tau(test_first_hitting_time[test_sample_index], 0, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
        "p_ev1 = CIF_K_tau(test_first_hitting_time[test_sample_index], 1, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
        "p_ev2 = CIF_K_tau(test_first_hitting_time[test_sample_index], 2, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
        "\n",
        "print(\"probability a prepay happens = %.3f\" % p_ev0)\n",
        "print(\"probability a default happens = %.3f\" % p_ev1)\n",
        "print(\"probability a full repay happens = %.3f\" % p_ev2)\n",
        "\n",
        "sum = p_ev0 + p_ev1 + p_ev2\n",
        "print(\"The probability anything happens = %.3f\" % sum)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU-El_hpNu-C"
      },
      "source": [
        "### 7.4 Probability that this loan will prepay, default or repay with less data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpVzoJRgNu-C"
      },
      "outputs": [],
      "source": [
        "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
        "\n",
        "delta = 6\n",
        "\n",
        "shortened_batch_data_length = test_batch_data_length - delta\n",
        "\n",
        "shortened_batch_data_length = shortened_batch_data_length.to(DEVICE)\n",
        "test_batch_data = test_batch_data.to(DEVICE)\n",
        "\n",
        "print(\"sample has length %d, but we concatenated to %d\" % (test_batch_data_length[test_sample_index], shortened_batch_data_length[test_sample_index]))\n",
        "\n",
        "test_output, test_first_hitting_time, test_attention_weights = DDHT(test_batch_data, shortened_batch_data_length)\n",
        "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
        "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
        "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
        "\n",
        "print(\"sample has length %d\" % shortened_batch_data_length[test_sample_index])\n",
        "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
        "\n",
        "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, shortened_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, shortened_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, shortened_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
        "\n",
        "plot_fht_and_cif(test_first_hitting_time[test_sample_index], shortened_batch_data_length[test_sample_index], MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxRc10ooNu-C"
      },
      "outputs": [],
      "source": [
        "test_sample_index = 2\n",
        "\n",
        "PREDICTION_DEPTH = 6\n",
        "\n",
        "from utils import plot_dynamic_risk_prediction\n",
        "\n",
        "#covariates_to_plot = [\"CURRENT_INTEREST_RATE\", \"CURRENT_LOAN_DELINQUENCY_STATUS\", \"LOAN_AGE\"]\n",
        "covariates_to_plot = allowed_covariates\n",
        "\n",
        "probability_of_event_0 = []\n",
        "probability_of_event_1 = []\n",
        "probability_of_event_2 = []\n",
        "covariate_series = []\n",
        "\n",
        "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(train_data_loader))\n",
        "\n",
        "original_test_batch_data = copy.deepcopy(test_batch_data)[test_sample_index].unsqueeze(0).to(DEVICE)\n",
        "original_length = copy.deepcopy(test_batch_data_length)[test_sample_index].unsqueeze(0).to(DEVICE)\n",
        "\n",
        "for covariate in covariates_to_plot:\n",
        "    covariate_series.append(test_batch_data[test_sample_index,:,allowed_covariates.index(covariate)].detach().cpu().numpy())\n",
        "\n",
        "original_length_integer = original_length.squeeze(0).item()\n",
        "for evaluation_time in range(original_length_integer):\n",
        "\n",
        "    delta = original_length_integer - evaluation_time\n",
        "    shortened_length = original_length - delta\n",
        "    \n",
        "    test_output, test_first_hitting_time, _ = DDHT(original_test_batch_data, shortened_length)\n",
        "    test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
        "    model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
        "    model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
        "\n",
        "    probability_of_event_0.append(CIF_K_tau(test_first_hitting_time[0], 0, evaluation_time + PREDICTION_DEPTH, shortened_length[0], MAX_LENGTH).item())\n",
        "    probability_of_event_1.append(CIF_K_tau(test_first_hitting_time[0], 1, evaluation_time + PREDICTION_DEPTH, shortened_length[0], MAX_LENGTH).item())\n",
        "    probability_of_event_2.append(CIF_K_tau(test_first_hitting_time[0], 2, evaluation_time + PREDICTION_DEPTH, shortened_length[0], MAX_LENGTH).item())\n",
        "\n",
        "test_output, test_first_hitting_time, _ = DDHT(original_test_batch_data, original_length)\n",
        "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
        "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
        "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
        "\n",
        "for evaluation_time in range(original_length_integer, MAX_LENGTH):\n",
        "    probability_of_event_0.append(CIF_K_tau(test_first_hitting_time[0], 0, evaluation_time, original_length[0], MAX_LENGTH).item())\n",
        "    probability_of_event_1.append(CIF_K_tau(test_first_hitting_time[0], 1, evaluation_time, original_length[0], MAX_LENGTH).item())\n",
        "    probability_of_event_2.append(CIF_K_tau(test_first_hitting_time[0], 2, evaluation_time, original_length[0], MAX_LENGTH).item())\n",
        "\n",
        "print(\"the original sample has length %d\" % shortened_batch_data_length[test_sample_index])\n",
        "\n",
        "PLOT_LENGTH = MAX_LENGTH\n",
        "DATA_LENGTH = original_length_integer\n",
        "plot_dynamic_risk_prediction(probability_of_event_0, probability_of_event_1, probability_of_event_2, covariates_to_plot, covariate_series, DATA_LENGTH, PLOT_LENGTH, MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ZWHe0INu-C"
      },
      "source": [
        "### 7.5 Sensitivity to certain covariates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGguVCo1Nu-D"
      },
      "outputs": [],
      "source": [
        "from utils import plot_gamma\n",
        "\n",
        "delta = 6 #This is variable\n",
        "\n",
        "min_gamma = torch.zeros(NUM_CAUSES, input_size)\n",
        "max_gamma = torch.zeros(NUM_CAUSES, input_size)\n",
        "\n",
        "for covariate_index in range(input_size):\n",
        "    sensitivity_batch_data, sensitivity_batch_data_length, sensitivity_batch_event, sensitivity_batch_tte = next(iter(test_data_loader))\n",
        "\n",
        "    #assuming it's minimum & maximum from the whole batch, not from a single sample\n",
        "    batch_min = torch.tensor(float('Inf'))\n",
        "    batch_max = (-1)*torch.tensor(float('Inf'))\n",
        "\n",
        "    #we have to iterate over it otherwise, we catch the zero's that are meant as NANs in our case\n",
        "    for sample, data_length in zip(sensitivity_batch_data, sensitivity_batch_data_length):\n",
        "        sample_min = torch.min(sample[:data_length,covariate_index])\n",
        "        sample_max = torch.max(sample[:data_length,covariate_index])\n",
        "\n",
        "        if sample_min < batch_min:\n",
        "            batch_min = sample_min\n",
        "\n",
        "        if sample_max > batch_max:\n",
        "            batch_max = sample_max\n",
        "\n",
        "    min_sensitivity_batch_data = copy.deepcopy(sensitivity_batch_data)\n",
        "    max_sensitivity_batch_data = copy.deepcopy(sensitivity_batch_data)\n",
        "\n",
        "    #for safety we iterate again, since we otherwise fill in the zero's that are meant as NANs\n",
        "    for sample_index, data_length in enumerate(sensitivity_batch_data_length):\n",
        "        min_sensitivity_batch_data[sample_index,:data_length, covariate_index] = batch_min\n",
        "        max_sensitivity_batch_data[sample_index,:data_length, covariate_index] = batch_max\n",
        "\n",
        "    min_sensitivity_batch_data = min_sensitivity_batch_data.to(DEVICE)\n",
        "    max_sensitivity_batch_data = max_sensitivity_batch_data.to(DEVICE)\n",
        "    sensitivity_batch_data_length = sensitivity_batch_data_length.to(DEVICE)\n",
        "\n",
        "    #for safety we iterate again, because the previous iteration might be absorbed by the one above that\n",
        "    for sample_index, data_length in enumerate(sensitivity_batch_data_length):\n",
        "        for cause_index in range(NUM_CAUSES):\n",
        "            evaluation_time = min(int(data_length.item()) + delta, MAX_LENGTH)\n",
        "\n",
        "            _, max_fht, _ = DDHT(max_sensitivity_batch_data[sample_index].unsqueeze(0), data_length)\n",
        "            _, min_fht, _ = DDHT(min_sensitivity_batch_data[sample_index].unsqueeze(0), data_length)\n",
        "\n",
        "            max_gamma[cause_index, covariate_index] += CIF_K_tau(max_fht[0], cause_index, evaluation_time, data_length, MAX_LENGTH).item()\n",
        "            min_gamma[cause_index, covariate_index] += CIF_K_tau(min_fht[0], cause_index, evaluation_time, data_length, MAX_LENGTH).item()\n",
        "\n",
        "\n",
        "gamma = (1/BATCH_SIZE)*(min_gamma - max_gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rk79yuX7Nu-D"
      },
      "outputs": [],
      "source": [
        "plot_gamma(gamma, y_lim=[-0.2, 0.2])\n",
        "\n",
        "print(\"The following covariate sensitivities are represented:\")\n",
        "for i, covariate in enumerate(allowed_covariates):\n",
        "    print(\"Index %d represents covariate %s\" % (i, covariate))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "18e97b579e8c0e40da8e2bba439fcd59aed88dbd21d3c026977017f366946867"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "main_FREDDIEMAC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}