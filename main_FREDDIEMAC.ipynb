{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "# SRC_PATH = '/content/gdrive/MyDrive/MP FEB/FREDDIEMAC'\n",
    "# DATA_SRC_PATH = SRC_PATH + \"/data\"\n",
    "# sys.path.append(SRC_PATH)\n",
    "\n",
    "# !pip install wandb -qqq\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "# !pip install dask[dataframe] -qqq\n",
    "# !pip install fastparquet python-snappy -qqq\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"main_FREDDIEMAC\", \n",
    "#     # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "#     #name=\"experiment 1\"\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     #config={\n",
    "#       #\"learning_rate\": 0.02,\n",
    "#       #\"architecture\": \"CNN\",\n",
    "#       #\"dataset\": \"CIFAR-100\",\n",
    "#       #\"epochs\": 10,}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SRC_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataframe contains a total of 1734900 entries\n",
      "This dataframe contains a total of 24614 loands\n"
     ]
    }
   ],
   "source": [
    "blumenstock_dynamic_types = {'LOAN_SEQUENCE_NUMBER': str, 'MONTHLY_REPORTING_PERIOD': str,'CURRENT_ACTUAL_UPB': float, 'CURRENT_LOAN_DELINQUENCY_STATUS': float, \n",
    "                    'CURRENT_INTEREST_RATE':float,'ELTV': float ,'LOAN_AGE': float, 'REMAINING_MONTHS_TO_LEGAL_MATURITY': float, 'CREDIT_SCORE': float,\n",
    "                    'DTI': float, 'LTV': float, 'BAL_REPAID': float, \n",
    "                    'LABEL': float, \"TIME_TO_EVENT\": float, 'ORIGINAL_INTEREST_RATE': float, 'ORIGINAL_UPB': float, 'TOTAL_OBSERVED_LENGTH': float}\n",
    "\n",
    "df_blumenstock_dynamic = dd.read_parquet(DATA_SRC_PATH + \"/blumenstock_dynamic_labeled_sample_orig_*.parquet.gzip\")\n",
    "df_blumenstock_dynamic = df_blumenstock_dynamic.astype(blumenstock_dynamic_types)\n",
    "\n",
    "print(\"This dataframe contains a total of %d entries\" % len(df_blumenstock_dynamic))\n",
    "print(\"This dataframe contains a total of %d loands\" % len(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalising raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_to_normalise = ['CURRENT_ACTUAL_UPB', 'CURRENT_LOAN_DELINQUENCY_STATUS', 'CURRENT_INTEREST_RATE', 'ELTV', 'LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY', 'CREDIT_SCORE',\n",
    "                            'DTI', 'LTV', 'BAL_REPAID', 'ORIGINAL_INTEREST_RATE', 'ORIGINAL_UPB']\n",
    "\n",
    "df_blumenstoch_dynamic_mean = df_blumenstock_dynamic[covariates_to_normalise].mean().compute()\n",
    "df_blumenstoch_std = df_blumenstock_dynamic[covariates_to_normalise].std().compute()\n",
    "\n",
    "df_blumenstock_dynamic[covariates_to_normalise] = (df_blumenstock_dynamic[covariates_to_normalise] - df_blumenstoch_dynamic_mean) / df_blumenstoch_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Splitting train, validation and test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train set contains 292400 entries\n",
      "Total validation set contains 34775 entries\n",
      "Total test set contains 18621 entries\n",
      "------------------------------------------------------------------\n",
      "Total train set contains 4097 loans\n",
      "Total validation set contains 513 loans\n",
      "Total test set contains 257 loans\n"
     ]
    }
   ],
   "source": [
    "#TODO THESE SETS ARE OVERLAPPING!!!\n",
    "\n",
    "AMOUNT_OF_TRAIN_LOANS = 2**12 + 1\n",
    "AMOUNT_OF_VALIDATE_LOANS = 2**9 + 1\n",
    "AMOUNT_OF_TEST_LOANS = 2**8 + 1\n",
    "\n",
    "train_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_TRAIN_LOANS, replace=False)\n",
    "validate_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_VALIDATE_LOANS, replace=False)\n",
    "test_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_TEST_LOANS, replace=False)\n",
    "\n",
    "validate_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(validate_choices)]\n",
    "train_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(train_choices)]\n",
    "test_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(test_choices)]\n",
    "\n",
    "print(\"Total train set contains %d entries\" % len(train_df_blumenstock))\n",
    "print(\"Total validation set contains %d entries\" % len(validate_df_blumenstock))\n",
    "print(\"Total test set contains %d entries\" % len(test_df_blumenstock))\n",
    "\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Total train set contains %d loans\" % len(train_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))\n",
    "print(\"Total validation set contains %d loans\" % len(validate_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))\n",
    "print(\"Total test set contains %d loans\" % len(test_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  128\n",
      "number of covariates =  12\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from FREDDIEMAC_main_data import FREDDIEMAC_main_dataset, FREDDIEMAC_main_dataloader\n",
    "\n",
    "BATCH_SIZE = 2**7\n",
    "\n",
    "allowed_covariates = ['CURRENT_ACTUAL_UPB', 'CURRENT_LOAN_DELINQUENCY_STATUS', 'CURRENT_INTEREST_RATE', 'ELTV', \n",
    "                             'LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY', 'CREDIT_SCORE', 'DTI', 'LTV', 'BAL_REPAID', \n",
    "                             'ORIGINAL_INTEREST_RATE', 'ORIGINAL_UPB']\n",
    "\n",
    "TOTAL_OBSERVED_LENGTH_covariate = 'TOTAL_OBSERVED_LENGTH'\n",
    "TIME_TO_EVENT_covariate ='TIME_TO_EVENT'\n",
    "LABEL_covariate = 'LABEL'\n",
    "\n",
    "random_state = 123\n",
    "augment = False\n",
    "data_augment_factor = 3\n",
    "\n",
    "print(\"batch_size = \", BATCH_SIZE)\n",
    "print(\"number of covariates = \", len(allowed_covariates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Creating train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset will contain 16388 samples\n",
      "This dataloader will deliver 128 batches\n",
      "torch.Size([128, 180, 12])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "augment_train_data = True\n",
    "\n",
    "train_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(train_df_blumenstock, \n",
    "                                                        allowed_covariates,\n",
    "                                                        TIME_TO_EVENT_covariate,\n",
    "                                                        TOTAL_OBSERVED_LENGTH_covariate,\n",
    "                                                        LABEL_covariate,\n",
    "                                                        frac_cases=1,\n",
    "                                                        random_state=random_state,\n",
    "                                                        test_set=False,\n",
    "                                                        augment=augment_train_data,\n",
    "                                                        data_augment_factor=data_augment_factor)\n",
    "\n",
    "print(\"This dataset will contain %d samples\" % len(train_FREDDIEMAC_raw_dataset))\n",
    "train_data_loader = FREDDIEMAC_main_dataloader(dataset=train_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"This dataloader will deliver %d batches\" % train_data_loader.get_max_iterations())\n",
    "batch_data, batch_data_length, batch_event, batch_tte = next(train_data_loader)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)\n",
    "print(batch_tte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Creating validate dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset will contain 513 samples\n",
      "This dataloader will deliver 4 batches\n",
      "torch.Size([128, 180, 12])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "augment_validate_data = False\n",
    "\n",
    "validate_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(validate_df_blumenstock, \n",
    "                                                            allowed_covariates,\n",
    "                                                            TIME_TO_EVENT_covariate,\n",
    "                                                            TOTAL_OBSERVED_LENGTH_covariate,\n",
    "                                                            LABEL_covariate,\n",
    "                                                            frac_cases=1,\n",
    "                                                            random_state=random_state,\n",
    "                                                            test_set=False,\n",
    "                                                            augment=augment_validate_data,\n",
    "                                                            data_augment_factor=data_augment_factor)\n",
    "\n",
    "print(\"This dataset will contain %d samples\" % len(validate_FREDDIEMAC_raw_dataset))\n",
    "validate_data_loader = FREDDIEMAC_main_dataloader(dataset=validate_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"This dataloader will deliver %d batches\" % validate_data_loader.get_max_iterations())\n",
    "batch_data, batch_data_length, batch_event, batch_tte = next(validate_data_loader)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)\n",
    "print(batch_tte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Creating a test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset will contain 257 samples\n",
      "This dataloader will deliver 2 batches\n",
      "torch.Size([128, 180, 12])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "augment_test_data = False\n",
    "\n",
    "test_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(test_df_blumenstock, \n",
    "                                                    allowed_covariates,\n",
    "                                                    TIME_TO_EVENT_covariate,\n",
    "                                                    TOTAL_OBSERVED_LENGTH_covariate,\n",
    "                                                    LABEL_covariate,\n",
    "                                                    frac_cases=1,\n",
    "                                                    random_state=random_state,\n",
    "                                                    test_set=False,\n",
    "                                                    augment=augment_test_data,\n",
    "                                                    data_augment_factor=data_augment_factor)\n",
    "\n",
    "print(\"This dataset will contain %d samples\" % len(test_FREDDIEMAC_raw_dataset))\n",
    "test_data_loader = FREDDIEMAC_main_dataloader(dataset=test_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"This dataloader will deliver %d batches\" % test_data_loader.get_max_iterations())\n",
    "batch_data, batch_data_length, batch_event, batch_tte = next(test_data_loader)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)\n",
    "print(batch_tte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch event= 0 --- batch_data_length= 34 --- batch_tte= 61\n",
      "batch event= 0 --- batch_data_length= 20 --- batch_tte= 29\n",
      "batch event= 0 --- batch_data_length= 27 --- batch_tte= 30\n",
      "batch event= 0 --- batch_data_length= 109 --- batch_tte= 109\n",
      "batch event= 3 --- batch_data_length= 122 --- batch_tte= 122\n",
      "batch event= 0 --- batch_data_length= 90 --- batch_tte= 92\n",
      "batch event= 0 --- batch_data_length= 98 --- batch_tte= 98\n",
      "batch event= 0 --- batch_data_length= 74 --- batch_tte= 74\n",
      "batch event= 0 --- batch_data_length= 30 --- batch_tte= 30\n",
      "batch event= 0 --- batch_data_length= 24 --- batch_tte= 24\n",
      "batch event= 0 --- batch_data_length= 33 --- batch_tte= 64\n",
      "batch event= 3 --- batch_data_length= 122 --- batch_tte= 122\n",
      "batch event= 0 --- batch_data_length= 25 --- batch_tte= 33\n",
      "batch event= 0 --- batch_data_length= 12 --- batch_tte= 12\n",
      "batch event= 0 --- batch_data_length= 60 --- batch_tte= 116\n",
      "batch event= 0 --- batch_data_length= 118 --- batch_tte= 118\n"
     ]
    }
   ],
   "source": [
    "batch_data, batch_data_length, batch_event, batch_tte = next(iter(train_data_loader))\n",
    "\n",
    "for i in range(min(BATCH_SIZE, 16)):\n",
    "    print(\"batch event= %d --- batch_data_length= %d --- batch_tte= %d\" % (batch_event[i], batch_data_length[i], batch_tte[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "from dynamicDeepHit import EncoderRNN, AttnDecoderRNN, CauseSpecificSubnetwork, DynamicDeepHit\n",
    "from losses import loss_1_batch, loss_2_batch, loss_3_batch\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "LEARNING_RATE_ENCODER = 0.001\n",
    "LEARNING_RATE_DECODER = 0.001\n",
    "LEARNING_RATE_CAUSESS = 0.0005\n",
    "\n",
    "LOSS_1_AMPLIFIER = 1\n",
    "LOSS_2_AMPLIFIER = 1\n",
    "LOSS_3_AMPLIFIER = 1\n",
    "\n",
    "RUN_VALIDATION_ROUND = True\n",
    "RUN_VALIDATION_ROUND_BATCHES_THRESHOLD = 2**2\n",
    "VAL_NUM_CASES_RUNTIME = BATCH_SIZE\n",
    "\n",
    "input_size = train_FREDDIEMAC_raw_dataset.get_num_covariates()\n",
    "output_size = input_size\n",
    "MAX_LENGTH = train_FREDDIEMAC_raw_dataset.get_max_length()\n",
    "\n",
    "NUM_CAUSES = 3\n",
    "SIGMA = 0.1\n",
    "\n",
    "rnn_state_size = 256\n",
    "\n",
    "encoder_fc_size = 512\n",
    "attention_fc_size = 512\n",
    "cause_fc_size = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "encoder = EncoderRNN(input_size, rnn_state_size, encoder_fc_size, output_size).to(DEVICE)\n",
    "decoder = AttnDecoderRNN(rnn_state_size, input_size, attention_fc_size).to(DEVICE)\n",
    "causess = CauseSpecificSubnetwork(rnn_state_size, cause_fc_size, MAX_LENGTH, NUM_CAUSES).to(DEVICE)\n",
    "DDHT = DynamicDeepHit(encoder, decoder, causess, MAX_LENGTH, DEVICE)\n",
    "\n",
    "# intialize optimizer\n",
    "optimizer_encoder = Adam(encoder.parameters(), lr=LEARNING_RATE_ENCODER)\n",
    "optimizer_decoder = Adam(decoder.parameters(), lr=LEARNING_RATE_DECODER)\n",
    "optimizer_causess = Adam(causess.parameters(), lr=LEARNING_RATE_CAUSESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Testing a sample before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "For the first sample of the test batch:\n",
      "sample has length 140\n",
      "the model predicts the event 0 at time 92\n",
      "probability of prepay event = 0.33\n",
      "probability of default event = 0.33\n",
      "probability of full repay event = 0.34\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import plot_fht_and_cif, plot_fht, plot_cif\n",
    "from losses import CIF_K_tau\n",
    "\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "test_batch_data = test_batch_data.to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
    "test_batch_event = test_batch_event.to(DEVICE)\n",
    "\n",
    "DDHT.eval()\n",
    "\n",
    "test_output, test_first_hitting_time, _ = DDHT(test_batch_data, test_batch_data_length)\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "\n",
    "DDHT.train()\n",
    "\n",
    "print(\"For the first sample of the test batch:\")\n",
    "print(\"sample has length %d\" % test_batch_data_length[0])\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[0], model_tte_prediction[0] + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 0, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 1, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 2, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[0], test_batch_data_length[0], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "{'train_loss1': 599.0536499023438, 'train_loss2': 54.136680603027344, 'train_loss3': 0.31019750237464905}\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n",
      "torch.Size([12])\n",
      "torch.Size([179, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128 [02:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d29c15af29ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m           \u001b[0mval_output_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_first_hitting_time_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDHT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_batch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_data_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m           \u001b[0mval_loss1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLOSS_1_AMPLIFIER\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_1_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_first_hitting_time_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_event\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_tte\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_data_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mVAL_NUM_CASES_RUNTIME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m           \u001b[0mval_loss2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLOSS_2_AMPLIFIER\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_2_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_first_hitting_time_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_event\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_tte\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_data_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_CAUSES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSIGMA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mVAL_NUM_CASES_RUNTIME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m           \u001b[0mval_loss3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLOSS_3_AMPLIFIER\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_3_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_output_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mVAL_NUM_CASES_RUNTIME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marij\\Desktop\\THESIS ECO\\DDHT\\DDHT_pytorch\\losses.py\u001b[0m in \u001b[0;36mloss_1_batch\u001b[1;34m(first_hitting_time_batch, event_batch, batch_tte, batch_data_length, MAX_LENGTH, device)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m#we don't know which event the subject will experience, but we know the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m# subject didn't experience any event before the hitting time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mcif\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCIF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_hitting_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_length\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Waarom is dit -2?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[1;31m#print(\"data_length=\", data_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m#print(\"CIF=\", CIF(first_hitting_time, data_length, MAX_LENGTH))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marij\\Desktop\\THESIS ECO\\DDHT\\DDHT_pytorch\\losses.py\u001b[0m in \u001b[0;36mCIF\u001b[1;34m(first_hitting_time, data_length, MAX_LENGTH)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamount_of_events\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mcif\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCIF_K\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_hitting_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;31m#print(\"CIF_K=\", CIF_K(first_hitting_time, event, data_length, MAX_LENGTH))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marij\\Desktop\\THESIS ECO\\DDHT\\DDHT_pytorch\\losses.py\u001b[0m in \u001b[0;36mCIF_K\u001b[1;34m(first_hitting_time, event_k, data_length, MAX_LENGTH)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mcif_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCIF_K_tau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_hitting_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcif_k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marij\\Desktop\\THESIS ECO\\DDHT\\DDHT_pytorch\\losses.py\u001b[0m in \u001b[0;36mCIF_K_tau\u001b[1;34m(first_hitting_time, event_k, tte, data_length, MAX_LENGTH)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mamount_of_events\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_hitting_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mnumerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_hitting_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamount_of_events\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevent_k\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtte\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdenomenator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_hitting_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamount_of_events\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata_length\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#print(\"numerator=\", numerator)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#print(\"denomenator=\", denomenator)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "#PATH = \"/content/gdrive/MyDrive/MP FEB/FREDDIEMAC/models/main_model_v1.pth\"\n",
    "\n",
    "# start training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "  epoch_loss = 0\n",
    "  train_epoch_val_loss = 0\n",
    "  train_epoch_val_loss1 = 0\n",
    "  train_epoch_val_loss2 = 0\n",
    "\n",
    "  for batch_number in trange(len(train_data_loader)):\n",
    "    data = next(train_data_loader)\n",
    "\n",
    "    batch_loss = 0\n",
    "\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    optimizer_causess.zero_grad()\n",
    "\n",
    "    batch_data, batch_data_length, batch_event, batch_tte = data\n",
    "    batch_data = batch_data.to(DEVICE)\n",
    "    batch_data_length = batch_data_length.to(DEVICE)\n",
    "    batch_event = batch_event.to(DEVICE)\n",
    "    batch_tte = batch_tte.to(DEVICE)\n",
    "    \n",
    "    output_batch, first_hitting_time_batch, _ = DDHT(batch_data, batch_data_length)\n",
    "\n",
    "    loss1 = LOSS_1_AMPLIFIER*loss_1_batch(first_hitting_time_batch, batch_event, batch_tte, batch_data_length, MAX_LENGTH, DEVICE)\n",
    "    loss2 = LOSS_2_AMPLIFIER*loss_2_batch(first_hitting_time_batch, batch_event, batch_tte, batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)\n",
    "    loss3 = LOSS_3_AMPLIFIER*loss_3_batch(output_batch, batch_data.detach())\n",
    "\n",
    "    batch_loss = loss1 + loss2 + loss3\n",
    "    batch_loss.backward()\n",
    "\n",
    "    epoch_loss += batch_loss.detach()\n",
    "\n",
    "    #wandb.log({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item(), \"train_loss3\": loss3.item()})\n",
    "    print({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item(), \"train_loss3\": loss3.item()})\n",
    "\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_decoder.step()\n",
    "    optimizer_causess.step()\n",
    "\n",
    "    if RUN_VALIDATION_ROUND and batch_number % 5 == 0:\n",
    "      # validation round\n",
    "      # torch.save(DDHT.state_dict(), PATH)\n",
    "      DDHT.eval()\n",
    "\n",
    "      val_epoch_val_loss = 0\n",
    "      val_epoch_val_loss1 = 0\n",
    "      val_epoch_val_loss2 = 0\n",
    "      val_epoch_val_loss3 = 0\n",
    "\n",
    "      VAL_NUM_CASES_RUNTIME = len(validate_data_loader)*BATCH_SIZE\n",
    "      for validation_batch_number in range(len(validate_data_loader)):\n",
    "        data = next(validate_data_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "          val_batch_data, val_batch_data_length, val_batch_event, val_batch_tte = next(iter(validate_data_loader))\n",
    "          val_batch_data = val_batch_data.to(DEVICE)\n",
    "          val_batch_data_length = val_batch_data_length.to(DEVICE)\n",
    "          val_batch_event = val_batch_event.to(DEVICE)\n",
    "          val_batch_tte = val_batch_tte.to(DEVICE)\n",
    "\n",
    "          val_output_batch, val_first_hitting_time_batch, _ = DDHT(val_batch_data, val_batch_data_length)\n",
    "\n",
    "          val_loss1 = LOSS_1_AMPLIFIER*loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, val_batch_data_length, MAX_LENGTH, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "          val_loss2 = LOSS_2_AMPLIFIER*loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, val_batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "          val_loss3 = LOSS_3_AMPLIFIER*loss_3_batch(val_output_batch, val_batch_data.detach())/VAL_NUM_CASES_RUNTIME\n",
    "\n",
    "          val_epoch_val_loss1 += val_loss1\n",
    "          val_epoch_val_loss2 += val_loss2\n",
    "          val_epoch_val_loss3 += val_loss3\n",
    "          val_epoch_val_loss = val_loss1 + val_loss2 + val_loss3\n",
    "\n",
    "      #wandb.log({\"val_epoch_val_loss1\": val_epoch_val_loss1.item(), \"val_epoch_val_loss2\": val_epoch_val_loss2.item(), \"val_epoch_val_loss3\": val_epoch_val_loss3.item(), \"val_epoch_val_loss\": val_epoch_val_loss.item()})\n",
    "      print({\"val_epoch_val_loss1\": val_epoch_val_loss1.item(), \"val_epoch_val_loss2\": val_epoch_val_loss2.item(), \"val_epoch_val_loss3\": val_epoch_val_loss3.item()})\n",
    "\n",
    "      DDHT.train()\n",
    "      # end validating round\n",
    "\n",
    "  #wandb.log({\"train_epoch_loss\": epoch_loss.item()})\n",
    "  #torch.save(DDHT.state_dict(), PATH)\n",
    "\n",
    "#wandb.finish() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"/content/gdrive/MyDrive/MP FEB/FREDDIEMAC/models/main_model_v1.pth\"\n",
    "PATH = \"models/main_model_local_v1.pth\"\n",
    "torch.save(DDHT.state_dict(), PATH)\n",
    "DDHT.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Testing a sample after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_fht_and_cif, plot_fht, plot_cif\n",
    "from losses import CIF_K_tau\n",
    "\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "test_batch_data = test_batch_data.to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
    "test_batch_event = test_batch_event.to(DEVICE)\n",
    "\n",
    "DDHT.eval()\n",
    "\n",
    "test_output, test_first_hitting_time, test_attention_weights = DDHT(test_batch_data, test_batch_data_length)\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "\n",
    "print(\"For the first sample of the test batch:\")\n",
    "print(\"sample has length %d\" % test_batch_data_length[test_sample_index])\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[test_sample_index], test_batch_data_length[test_sample_index], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Testing more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_sample_index in range(2**3):\n",
    "  print(\"--------------------------------------------\")\n",
    "  print(\"sample will experiece event: %d - at time %d\" % (test_batch_event[test_sample_index], test_batch_data_length[test_sample_index]))\n",
    "  print(\"model will predict event:    %d - at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Plotting a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = test_batch_event.flatten().cpu()\n",
    "b = model_event_prediction.cpu()\n",
    "\n",
    "cm = confusion_matrix(a, b)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['prepay', 'default', 'repay'])\n",
    "accuracy = cm.diagonal()/cm.sum(axis=1)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Analysing the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Which attention weights are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_attention_weights\n",
    "plot_attention_weights(test_attention_weights[test_sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_attention_weights_batch\n",
    "\n",
    "plot_attention_weights_batch(test_attention_weights, test_batch_data_length, normalised=True, y_lim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 What does the model do when we mask some covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "test_batch_data = test_batch_data.to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
    "test_batch_event = test_batch_event.to(DEVICE)\n",
    "\n",
    "masked_data = copy.deepcopy(test_batch_data)\n",
    "masked_covariates = ['LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY']\n",
    "\n",
    "for covariate in masked_covariates:\n",
    "  masked_data[:,:,allowed_covariates.index(covariate)] = 0.0\n",
    "\n",
    "masked_data = masked_data.to(DEVICE)\n",
    "\n",
    "test_output, test_first_hitting_time, test_attention_weights = DDHT(masked_data, test_batch_data_length)\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "\n",
    "\n",
    "print(\"from the allowed covariates: \", allowed_covariates)\n",
    "print(\"we masked the following covariates: \", masked_covariates)\n",
    "\n",
    "print(\"For the first sample of the test batch:\")\n",
    "print(\"sample has length %d\" % test_batch_data_length[test_sample_index])\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[test_sample_index], test_batch_data_length[test_sample_index], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Probability that this loan will prepay, default or repay in the comming \"delta\" months?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 6  #This is variable\n",
    "\n",
    "evaluation_time = min(int(test_batch_data_length[test_sample_index].item()) + delta, MAX_LENGTH)\n",
    "print(\"In the comming %d months, the probability that a specific event happens is:\" % delta)\n",
    "\n",
    "p_ev0 = CIF_K_tau(test_first_hitting_time[test_sample_index], 0, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
    "p_ev1 = CIF_K_tau(test_first_hitting_time[test_sample_index], 1, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
    "p_ev2 = CIF_K_tau(test_first_hitting_time[test_sample_index], 2, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
    "\n",
    "print(\"probability a prepay happens = %.3f\" % p_ev0)\n",
    "print(\"probability a default happens = %.3f\" % p_ev1)\n",
    "print(\"probability a full repay happens = %.3f\" % p_ev2)\n",
    "\n",
    "sum = p_ev0 + p_ev1 + p_ev2\n",
    "print(\"The probability anything happens = %.3f\" % sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Probability that this loan will prepay, default or repay with less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "delta = 6 #This is variable\n",
    "\n",
    "shortened_length = test_batch_data_length - delta\n",
    "shortened_test_batch_data = copy.deepcopy(test_batch_data)\n",
    "\n",
    "# we iterate over the whole batch and shorten their lengths accordingly\n",
    "for i in range(len(test_batch_data_length)):\n",
    "    left_censoring_index = int(shortened_length[i].item())\n",
    "    right_censoring_index = int(test_batch_data_length[i].item())\n",
    "    shortened_test_batch_data[i, left_censoring_index:right_censoring_index] = torch.zeros(delta, input_size)\n",
    "    shortened_test_batch_data_length = copy.deepcopy(shortened_length)\n",
    "\n",
    "shortened_test_batch_data = shortened_test_batch_data.to(DEVICE)\n",
    "shortened_test_batch_data_length = shortened_test_batch_data_length.to(DEVICE)\n",
    "test_batch_event = test_batch_event.to(DEVICE)\n",
    "\n",
    "print(\"sample has length %d, but we concatenated to %d\" % (test_batch_data_length[test_sample_index], shortened_test_batch_data_length[test_sample_index]))\n",
    "\n",
    "test_output, test_first_hitting_time, test_attention_weights = DDHT(masked_data, shortened_test_batch_data_length)\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "\n",
    "print(\"For the first sample of the test batch:\")\n",
    "print(\"sample has length %d\" % shortened_test_batch_data_length[test_sample_index])\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, shortened_test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, shortened_test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, shortened_test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[test_sample_index], shortened_test_batch_data_length[test_sample_index], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Sensitivity to certain covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_gamma\n",
    "\n",
    "delta = 6 #This is variable\n",
    "\n",
    "min_gamma = torch.zeros(NUM_CAUSES, input_size)\n",
    "max_gamma = torch.zeros(NUM_CAUSES, input_size)\n",
    "\n",
    "for covariate_index in range(input_size):\n",
    "    sensitivity_batch_data, sensitivity_batch_data_length, sensitivity_batch_event, sensitivity_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "    #assuming it's minimum & maximum from the whole batch, not from a single sample\n",
    "    batch_min = torch.tensor(float('Inf'))\n",
    "    batch_max = (-1)*torch.tensor(float('Inf'))\n",
    "\n",
    "    #we have to iterate over it otherwise, we catch the zero's that are meant as NANs in our case\n",
    "    for sample, data_length in zip(sensitivity_batch_data, sensitivity_batch_data_length):\n",
    "        sample_min = torch.min(sample[:data_length,covariate_index])\n",
    "        sample_max = torch.max(sample[:data_length,covariate_index])\n",
    "\n",
    "        if sample_min < batch_min:\n",
    "            batch_min = sample_min\n",
    "\n",
    "        if sample_max > batch_max:\n",
    "            batch_max = sample_max\n",
    "\n",
    "    min_sensitivity_batch_data = copy.deepcopy(sensitivity_batch_data)\n",
    "    max_sensitivity_batch_data = copy.deepcopy(sensitivity_batch_data)\n",
    "\n",
    "    #for safety we iterate again, since we otherwise fill in the zero's that are meant as NANs\n",
    "    for sample_index, data_length in enumerate(sensitivity_batch_data_length):\n",
    "        min_sensitivity_batch_data[sample_index,:data_length, covariate_index] = batch_min\n",
    "        max_sensitivity_batch_data[sample_index,:data_length, covariate_index] = batch_max\n",
    "\n",
    "    min_sensitivity_batch_data = min_sensitivity_batch_data.to(DEVICE)\n",
    "    max_sensitivity_batch_data = max_sensitivity_batch_data.to(DEVICE)\n",
    "    sensitivity_batch_data_length = sensitivity_batch_data_length.to(DEVICE)\n",
    "\n",
    "    #for safety we iterate again, because the previous iteration might be absorbed by the one above that\n",
    "    for sample_index, data_length in enumerate(sensitivity_batch_data_length):\n",
    "        for cause_index in range(NUM_CAUSES):\n",
    "            evaluation_time = min(int(data_length.item()) + delta, MAX_LENGTH)\n",
    "\n",
    "            _, max_fht, _ = DDHT(max_sensitivity_batch_data[sample_index].unsqueeze(0), data_length)\n",
    "            _, min_fht, _ = DDHT(min_sensitivity_batch_data[sample_index].unsqueeze(0), data_length)\n",
    "\n",
    "            max_gamma[cause_index, covariate_index] += CIF_K_tau(max_fht[0], cause_index, evaluation_time, data_length, MAX_LENGTH).item()\n",
    "            min_gamma[cause_index, covariate_index] += CIF_K_tau(min_fht[0], cause_index, evaluation_time, data_length, MAX_LENGTH).item()\n",
    "\n",
    "\n",
    "gamma = (1/BATCH_SIZE)*(min_gamma - max_gamma)\n",
    "plot_gamma(gamma, y_lim=[-1, 1])\n",
    "\n",
    "print(\"The following covariate sensitivities are represented:\")\n",
    "for i, covariate in enumerate(allowed_covariates):\n",
    "    print(\"Index %d represents covariate %s\" % (i, covariate))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e97b579e8c0e40da8e2bba439fcd59aed88dbd21d3c026977017f366946867"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
