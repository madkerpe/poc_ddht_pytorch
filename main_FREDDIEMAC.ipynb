{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/gdrive', force_remount=True)\n",
    "# SRC_PATH = '/content/gdrive/MyDrive/MP FEB/FREDDIEMAC'\n",
    "# DATA_SRC_PATH = SRC_PATH + \"/data\"\n",
    "# sys.path.append(SRC_PATH)\n",
    "\n",
    "# !pip install wandb -qqq\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "# !pip install dask[dataframe] -qqq\n",
    "# !pip install fastparquet python-snappy -qqq\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"main_FREDDIEMAC\", \n",
    "#     # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "#     #name=\"experiment 1\"\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     #config={\n",
    "#       #\"learning_rate\": 0.02,\n",
    "#       #\"architecture\": \"CNN\",\n",
    "#       #\"dataset\": \"CIFAR-100\",\n",
    "#       #\"epochs\": 10,}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SRC_PATH = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blumenstock_dynamic_types = {'LOAN_SEQUENCE_NUMBER': str, 'MONTHLY_REPORTING_PERIOD': str,'CURRENT_ACTUAL_UPB': float, 'CURRENT_LOAN_DELINQUENCY_STATUS': float, \n",
    "                    'CURRENT_INTEREST_RATE':float,'ELTV': float ,'LOAN_AGE': float, 'REMAINING_MONTHS_TO_LEGAL_MATURITY': float, 'CREDIT_SCORE': float,\n",
    "                    'DTI': float, 'LTV': float, 'BAL_REPAID': float, \n",
    "                    'LABEL': float, \"TIME_TO_EVENT\": float, 'ORIGINAL_INTEREST_RATE': float, 'ORIGINAL_UPB': float, 'TOTAL_OBSERVED_LENGTH': float}\n",
    "\n",
    "df_blumenstock_dynamic = dd.read_parquet(DATA_SRC_PATH + \"/blumenstock_dynamic_labeled_sample_orig_*.parquet.gzip\")\n",
    "df_blumenstock_dynamic = df_blumenstock_dynamic.astype(blumenstock_dynamic_types)\n",
    "\n",
    "print(\"This dataframe contains a total of %d entries\" % len(df_blumenstock_dynamic))\n",
    "print(\"This dataframe contains a total of %d loands\" % len(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalising raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_to_normalise = ['CURRENT_ACTUAL_UPB', 'CURRENT_LOAN_DELINQUENCY_STATUS', 'CURRENT_INTEREST_RATE', 'ELTV', 'LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY', 'CREDIT_SCORE',\n",
    "                            'DTI', 'LTV', 'BAL_REPAID', 'ORIGINAL_INTEREST_RATE', 'ORIGINAL_UPB']\n",
    "\n",
    "df_blumenstoch_dynamic_mean = df_blumenstock_dynamic[covariates_to_normalise].mean().compute()\n",
    "df_blumenstoch_std = df_blumenstock_dynamic[covariates_to_normalise].std().compute()\n",
    "\n",
    "df_blumenstock_dynamic[covariates_to_normalise] = (df_blumenstock_dynamic[covariates_to_normalise] - df_blumenstoch_dynamic_mean) / df_blumenstoch_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Splitting train, validation and test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO THESE SETS ARE OVERLAPPING!!!\n",
    "\n",
    "AMOUNT_OF_TRAIN_LOANS = 2**12 + 1\n",
    "AMOUNT_OF_VALIDATE_LOANS = 2**9 + 1\n",
    "AMOUNT_OF_TEST_LOANS = 2**8 + 1\n",
    "\n",
    "train_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_TRAIN_LOANS, replace=False)\n",
    "validate_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_VALIDATE_LOANS, replace=False)\n",
    "test_choices = np.random.choice(df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].unique(), AMOUNT_OF_TEST_LOANS, replace=False)\n",
    "\n",
    "validate_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(validate_choices)]\n",
    "train_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(train_choices)]\n",
    "test_df_blumenstock = df_blumenstock_dynamic[df_blumenstock_dynamic[\"LOAN_SEQUENCE_NUMBER\"].isin(test_choices)]\n",
    "\n",
    "print(\"Total train set contains %d entries\" % len(train_df_blumenstock))\n",
    "print(\"Total validation set contains %d entries\" % len(validate_df_blumenstock))\n",
    "print(\"Total test set contains %d entries\" % len(test_df_blumenstock))\n",
    "\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Total train set contains %d loans\" % len(train_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))\n",
    "print(\"Total validation set contains %d loans\" % len(validate_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))\n",
    "print(\"Total test set contains %d loans\" % len(test_df_blumenstock[\"LOAN_SEQUENCE_NUMBER\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from FREDDIEMAC_main_data import FREDDIEMAC_main_dataset, FREDDIEMAC_main_dataloader\n",
    "\n",
    "BATCH_SIZE = 2**7\n",
    "\n",
    "allowed_covariates = ['CURRENT_ACTUAL_UPB', 'CURRENT_LOAN_DELINQUENCY_STATUS', 'CURRENT_INTEREST_RATE', 'ELTV', \n",
    "                             'LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY', 'CREDIT_SCORE', 'DTI', 'LTV', 'BAL_REPAID', \n",
    "                             'ORIGINAL_INTEREST_RATE', 'ORIGINAL_UPB']\n",
    "\n",
    "TOTAL_OBSERVED_LENGTH_covariate = 'TOTAL_OBSERVED_LENGTH'\n",
    "TIME_TO_EVENT_covariate ='TIME_TO_EVENT'\n",
    "LABEL_covariate = 'LABEL'\n",
    "\n",
    "random_state = 123\n",
    "augment = False\n",
    "data_augment_factor = 3\n",
    "\n",
    "print(\"batch_size = \", BATCH_SIZE)\n",
    "print(\"number of covariates = \", len(allowed_covariates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Creating train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_train_data = True\n",
    "\n",
    "train_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(train_df_blumenstock, \n",
    "                                                        allowed_covariates,\n",
    "                                                        TIME_TO_EVENT_covariate,\n",
    "                                                        TOTAL_OBSERVED_LENGTH_covariate,\n",
    "                                                        LABEL_covariate,\n",
    "                                                        frac_cases=1,\n",
    "                                                        random_state=random_state,\n",
    "                                                        test_set=False,\n",
    "                                                        augment=augment_train_data,\n",
    "                                                        data_augment_factor=data_augment_factor)\n",
    "\n",
    "print(\"This dataset will contain %d samples\" % len(train_FREDDIEMAC_raw_dataset))\n",
    "train_data_loader = FREDDIEMAC_main_dataloader(dataset=train_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"This dataloader will deliver %d batches\" % train_data_loader.get_max_iterations())\n",
    "batch_data, batch_data_length, batch_event, batch_tte = next(train_data_loader)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)\n",
    "print(batch_tte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Creating validate dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_validate_data = False\n",
    "\n",
    "validate_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(validate_df_blumenstock, \n",
    "                                                            allowed_covariates,\n",
    "                                                            TIME_TO_EVENT_covariate,\n",
    "                                                            TOTAL_OBSERVED_LENGTH_covariate,\n",
    "                                                            LABEL_covariate,\n",
    "                                                            frac_cases=1,\n",
    "                                                            random_state=random_state,\n",
    "                                                            test_set=False,\n",
    "                                                            augment=augment_validate_data,\n",
    "                                                            data_augment_factor=data_augment_factor)\n",
    "\n",
    "print(\"This dataset will contain %d samples\" % len(validate_FREDDIEMAC_raw_dataset))\n",
    "validate_data_loader = FREDDIEMAC_main_dataloader(dataset=validate_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"This dataloader will deliver %d batches\" % validate_data_loader.get_max_iterations())\n",
    "batch_data, batch_data_length, batch_event, batch_tte = next(validate_data_loader)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)\n",
    "print(batch_tte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Creating a test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_test_data = False\n",
    "\n",
    "test_FREDDIEMAC_raw_dataset = FREDDIEMAC_main_dataset(test_df_blumenstock, \n",
    "                                                    allowed_covariates,\n",
    "                                                    TIME_TO_EVENT_covariate,\n",
    "                                                    TOTAL_OBSERVED_LENGTH_covariate,\n",
    "                                                    LABEL_covariate,\n",
    "                                                    frac_cases=1,\n",
    "                                                    random_state=random_state,\n",
    "                                                    test_set=False,\n",
    "                                                    augment=augment_test_data,\n",
    "                                                    data_augment_factor=data_augment_factor)\n",
    "\n",
    "print(\"This dataset will contain %d samples\" % len(test_FREDDIEMAC_raw_dataset))\n",
    "test_data_loader = FREDDIEMAC_main_dataloader(dataset=test_FREDDIEMAC_raw_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"This dataloader will deliver %d batches\" % test_data_loader.get_max_iterations())\n",
    "batch_data, batch_data_length, batch_event, batch_tte = next(test_data_loader)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)\n",
    "print(batch_tte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_data_length, batch_event, batch_tte = next(iter(train_data_loader))\n",
    "\n",
    "for i in range(min(BATCH_SIZE, 16)):\n",
    "    print(\"batch event= %d --- batch_data_length= %d --- batch_tte= %d\" % (batch_event[i], batch_data_length[i], batch_tte[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "from dynamicDeepHit import EncoderRNN, AttnDecoderRNN, CauseSpecificSubnetwork, DynamicDeepHit\n",
    "from losses import loss_1_batch, loss_2_batch, loss_3_batch\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "LEARNING_RATE_ENCODER = 0.001\n",
    "LEARNING_RATE_DECODER = 0.001\n",
    "LEARNING_RATE_CAUSESS = 0.0005\n",
    "\n",
    "LOSS_1_AMPLIFIER = 1\n",
    "LOSS_2_AMPLIFIER = 1\n",
    "LOSS_3_AMPLIFIER = 1\n",
    "\n",
    "RUN_VALIDATION_ROUND = True\n",
    "RUN_VALIDATION_ROUND_BATCHES_THRESHOLD = 2**2\n",
    "VAL_NUM_CASES_RUNTIME = BATCH_SIZE\n",
    "\n",
    "input_size = train_FREDDIEMAC_raw_dataset.get_num_covariates()\n",
    "output_size = input_size\n",
    "MAX_LENGTH = train_FREDDIEMAC_raw_dataset.get_max_length()\n",
    "\n",
    "NUM_CAUSES = 3\n",
    "SIGMA = 0.1\n",
    "\n",
    "rnn_state_size = 256\n",
    "\n",
    "encoder_fc_size = 512\n",
    "attention_fc_size = 512\n",
    "cause_fc_size = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "encoder = EncoderRNN(input_size, rnn_state_size, encoder_fc_size, output_size).to(DEVICE)\n",
    "decoder = AttnDecoderRNN(rnn_state_size, input_size, attention_fc_size).to(DEVICE)\n",
    "causess = CauseSpecificSubnetwork(rnn_state_size, input_size, cause_fc_size, MAX_LENGTH, NUM_CAUSES).to(DEVICE)\n",
    "DDHT = DynamicDeepHit(encoder, decoder, causess, MAX_LENGTH, DEVICE)\n",
    "\n",
    "# intialize optimizer\n",
    "optimizer_encoder = Adam(encoder.parameters(), lr=LEARNING_RATE_ENCODER)\n",
    "optimizer_decoder = Adam(decoder.parameters(), lr=LEARNING_RATE_DECODER)\n",
    "optimizer_causess = Adam(causess.parameters(), lr=LEARNING_RATE_CAUSESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Testing a sample before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_fht_and_cif, plot_fht, plot_cif\n",
    "from losses import CIF_K_tau\n",
    "\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "test_batch_data = test_batch_data.to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
    "test_batch_event = test_batch_event.to(DEVICE)\n",
    "\n",
    "DDHT.eval()\n",
    "\n",
    "test_output, test_first_hitting_time, _ = DDHT(test_batch_data, test_batch_data_length)\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "\n",
    "DDHT.train()\n",
    "\n",
    "print(\"For the first sample of the test batch:\")\n",
    "print(\"sample has length %d\" % test_batch_data_length[0])\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[0], model_tte_prediction[0] + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 0, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 1, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 2, MAX_LENGTH, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[0], test_batch_data_length[0], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "#PATH = \"/content/gdrive/MyDrive/MP FEB/FREDDIEMAC/models/main_model_v1.pth\"\n",
    "\n",
    "# start training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "  epoch_loss = 0\n",
    "  train_epoch_val_loss = 0\n",
    "  train_epoch_val_loss1 = 0\n",
    "  train_epoch_val_loss2 = 0\n",
    "\n",
    "  for batch_number in trange(len(train_data_loader)):\n",
    "    data = next(train_data_loader)\n",
    "\n",
    "    batch_loss = 0\n",
    "\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    optimizer_causess.zero_grad()\n",
    "\n",
    "    batch_data, batch_data_length, batch_event, batch_tte = data\n",
    "    batch_data = batch_data.to(DEVICE)\n",
    "    batch_data_length = batch_data_length.to(DEVICE)\n",
    "    batch_event = batch_event.to(DEVICE)\n",
    "    batch_tte = batch_tte.to(DEVICE)\n",
    "    \n",
    "    output_batch, first_hitting_time_batch, _ = DDHT(batch_data, batch_data_length)\n",
    "\n",
    "    loss1 = LOSS_1_AMPLIFIER*loss_1_batch(first_hitting_time_batch, batch_event, batch_tte, batch_data_length, MAX_LENGTH, DEVICE)\n",
    "    loss2 = LOSS_2_AMPLIFIER*loss_2_batch(first_hitting_time_batch, batch_event, batch_tte, batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)\n",
    "    loss3 = LOSS_3_AMPLIFIER*loss_3_batch(output_batch, batch_data.detach())\n",
    "\n",
    "    batch_loss = loss1 + loss2 + loss3\n",
    "    batch_loss.backward()\n",
    "\n",
    "    epoch_loss += batch_loss.detach()\n",
    "\n",
    "    #wandb.log({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item(), \"train_loss3\": loss3.item()})\n",
    "    print({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item(), \"train_loss3\": loss3.item()})\n",
    "\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_decoder.step()\n",
    "    optimizer_causess.step()\n",
    "\n",
    "    if RUN_VALIDATION_ROUND and batch_number % 5 == 0:\n",
    "      # validation round\n",
    "      # torch.save(DDHT.state_dict(), PATH)\n",
    "      DDHT.eval()\n",
    "\n",
    "      val_epoch_val_loss = 0\n",
    "      val_epoch_val_loss1 = 0\n",
    "      val_epoch_val_loss2 = 0\n",
    "      val_epoch_val_loss3 = 0\n",
    "\n",
    "      VAL_NUM_CASES_RUNTIME = len(validate_data_loader)*BATCH_SIZE\n",
    "      for validation_batch_number in range(len(validate_data_loader)):\n",
    "        data = next(validate_data_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "          val_batch_data, val_batch_data_length, val_batch_event, val_batch_tte = next(iter(validate_data_loader))\n",
    "          val_batch_data = val_batch_data.to(DEVICE)\n",
    "          val_batch_data_length = val_batch_data_length.to(DEVICE)\n",
    "          val_batch_event = val_batch_event.to(DEVICE)\n",
    "          val_batch_tte = val_batch_tte.to(DEVICE)\n",
    "\n",
    "          val_output_batch, val_first_hitting_time_batch, _ = DDHT(val_batch_data, val_batch_data_length)\n",
    "\n",
    "          val_loss1 = LOSS_1_AMPLIFIER*loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, val_batch_data_length, MAX_LENGTH, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "          val_loss2 = LOSS_2_AMPLIFIER*loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, val_batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "          val_loss3 = LOSS_3_AMPLIFIER*loss_3_batch(val_output_batch, val_batch_data.detach())/VAL_NUM_CASES_RUNTIME\n",
    "\n",
    "          val_epoch_val_loss1 += val_loss1\n",
    "          val_epoch_val_loss2 += val_loss2\n",
    "          val_epoch_val_loss3 += val_loss3\n",
    "          val_epoch_val_loss = val_loss1 + val_loss2 + val_loss3\n",
    "\n",
    "      #wandb.log({\"val_epoch_val_loss1\": val_epoch_val_loss1.item(), \"val_epoch_val_loss2\": val_epoch_val_loss2.item(), \"val_epoch_val_loss3\": val_epoch_val_loss3.item(), \"val_epoch_val_loss\": val_epoch_val_loss.item()})\n",
    "      print({\"val_epoch_val_loss1\": val_epoch_val_loss1.item(), \"val_epoch_val_loss2\": val_epoch_val_loss2.item(), \"val_epoch_val_loss3\": val_epoch_val_loss3.item()})\n",
    "\n",
    "      DDHT.train()\n",
    "      # end validating round\n",
    "\n",
    "  #wandb.log({\"train_epoch_loss\": epoch_loss.item()})\n",
    "  #torch.save(DDHT.state_dict(), PATH)\n",
    "\n",
    "#wandb.finish() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"/content/gdrive/MyDrive/MP FEB/FREDDIEMAC/models/main_model_v1.pth\"\n",
    "PATH = \"models/main_model_local_v1.pth\"\n",
    "torch.save(DDHT.state_dict(), PATH)\n",
    "DDHT.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Testing a sample after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_fht_and_cif, plot_fht, plot_cif\n",
    "from losses import CIF_K_tau\n",
    "\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "test_batch_data = test_batch_data.to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
    "test_batch_event = test_batch_event.to(DEVICE)\n",
    "\n",
    "DDHT.eval()\n",
    "\n",
    "test_output, test_first_hitting_time, test_attention_weights = DDHT(test_batch_data, test_batch_data_length)\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "\n",
    "print(\"For the first sample of the test batch:\")\n",
    "print(\"sample has length %d\" % test_batch_data_length[test_sample_index])\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[test_sample_index], test_batch_data_length[test_sample_index], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Testing more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_sample_index in range(2**3):\n",
    "  print(\"--------------------------------------------\")\n",
    "  print(\"sample will experiece event: %d - at time %d\" % (test_batch_event[test_sample_index], test_batch_data_length[test_sample_index]))\n",
    "  print(\"model will predict event:    %d - at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Plotting a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = test_batch_event.flatten().cpu()\n",
    "b = model_event_prediction.cpu()\n",
    "\n",
    "cm = confusion_matrix(a, b)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['prepay', 'default', 'repay'])\n",
    "accuracy = cm.diagonal()/cm.sum(axis=1)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Analysing the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Which attention weights are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_attention_weights\n",
    "plot_attention_weights(test_attention_weights[test_sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_attention_weights_batch\n",
    "\n",
    "plot_attention_weights_batch(test_attention_weights, test_batch_data_length, normalised=True, y_lim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 What does the model do when we mask some covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "test_batch_data = test_batch_data.to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.to(DEVICE)\n",
    "test_batch_event = test_batch_event.to(DEVICE)\n",
    "\n",
    "masked_data = copy.deepcopy(test_batch_data)\n",
    "masked_covariates = ['LOAN_AGE', 'REMAINING_MONTHS_TO_LEGAL_MATURITY']\n",
    "\n",
    "for covariate in masked_covariates:\n",
    "  masked_data[:,:,allowed_covariates.index(covariate)] = 0.0\n",
    "\n",
    "masked_data = masked_data.to(DEVICE)\n",
    "\n",
    "test_output, test_first_hitting_time, test_attention_weights = DDHT(masked_data, test_batch_data_length)\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "\n",
    "\n",
    "print(\"from the allowed covariates: \", allowed_covariates)\n",
    "print(\"we masked the following covariates: \", masked_covariates)\n",
    "\n",
    "print(\"For the first sample of the test batch:\")\n",
    "print(\"sample has length %d\" % test_batch_data_length[test_sample_index])\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[test_sample_index], test_batch_data_length[test_sample_index], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Probability that this loan will prepay, default or repay in the comming \"delta\" months?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 6  #This is variable\n",
    "\n",
    "evaluation_time = min(int(test_batch_data_length[test_sample_index].item()) + delta, MAX_LENGTH)\n",
    "print(\"In the comming %d months, the probability that a specific event happens is:\" % delta)\n",
    "\n",
    "p_ev0 = CIF_K_tau(test_first_hitting_time[test_sample_index], 0, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
    "p_ev1 = CIF_K_tau(test_first_hitting_time[test_sample_index], 1, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
    "p_ev2 = CIF_K_tau(test_first_hitting_time[test_sample_index], 2, evaluation_time, test_batch_data_length[test_sample_index], MAX_LENGTH).item()\n",
    "\n",
    "print(\"probability a prepay happens = %.3f\" % p_ev0)\n",
    "print(\"probability a default happens = %.3f\" % p_ev1)\n",
    "print(\"probability a full repay happens = %.3f\" % p_ev2)\n",
    "\n",
    "sum = p_ev0 + p_ev1 + p_ev2\n",
    "print(\"The probability anything happens = %.3f\" % sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Probability that this loan will prepay, default or repay with less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "delta = 6 #This is variable\n",
    "\n",
    "shortened_length = test_batch_data_length - delta\n",
    "shortened_test_batch_data = copy.deepcopy(test_batch_data)\n",
    "\n",
    "# we iterate over the whole batch and shorten their lengths accordingly\n",
    "for i in range(len(test_batch_data_length)):\n",
    "    left_censoring_index = int(shortened_length[i].item())\n",
    "    right_censoring_index = int(test_batch_data_length[i].item())\n",
    "    shortened_test_batch_data[i, left_censoring_index:right_censoring_index] = torch.zeros(delta, input_size)\n",
    "    shortened_test_batch_data_length = copy.deepcopy(shortened_length)\n",
    "\n",
    "shortened_test_batch_data = shortened_test_batch_data.to(DEVICE)\n",
    "shortened_test_batch_data_length = shortened_test_batch_data_length.to(DEVICE)\n",
    "test_batch_event = test_batch_event.to(DEVICE)\n",
    "\n",
    "print(\"sample has length %d, but we concatenated to %d\" % (test_batch_data_length[test_sample_index], shortened_test_batch_data_length[test_sample_index]))\n",
    "\n",
    "test_output, test_first_hitting_time, test_attention_weights = DDHT(masked_data, shortened_test_batch_data_length)\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax(dim=1)\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "\n",
    "print(\"For the first sample of the test batch:\")\n",
    "print(\"sample has length %d\" % shortened_test_batch_data_length[test_sample_index])\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction[test_sample_index], model_tte_prediction[test_sample_index] + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 0, MAX_LENGTH, shortened_test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 1, MAX_LENGTH, shortened_test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[test_sample_index], 2, MAX_LENGTH, shortened_test_batch_data_length[test_sample_index], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[test_sample_index], shortened_test_batch_data_length[test_sample_index], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Sensitivity to certain covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_gamma\n",
    "\n",
    "delta = 6 #This is variable\n",
    "\n",
    "min_gamma = torch.zeros(NUM_CAUSES, input_size)\n",
    "max_gamma = torch.zeros(NUM_CAUSES, input_size)\n",
    "\n",
    "for covariate_index in range(input_size):\n",
    "    sensitivity_batch_data, sensitivity_batch_data_length, sensitivity_batch_event, sensitivity_batch_tte = next(iter(test_data_loader))\n",
    "\n",
    "    #assuming it's minimum & maximum from the whole batch, not from a single sample\n",
    "    batch_min = torch.tensor(float('Inf'))\n",
    "    batch_max = (-1)*torch.tensor(float('Inf'))\n",
    "\n",
    "    #we have to iterate over it otherwise, we catch the zero's that are meant as NANs in our case\n",
    "    for sample, data_length in zip(sensitivity_batch_data, sensitivity_batch_data_length):\n",
    "        sample_min = torch.min(sample[:data_length,covariate_index])\n",
    "        sample_max = torch.max(sample[:data_length,covariate_index])\n",
    "\n",
    "        if sample_min < batch_min:\n",
    "            batch_min = sample_min\n",
    "\n",
    "        if sample_max > batch_max:\n",
    "            batch_max = sample_max\n",
    "\n",
    "    min_sensitivity_batch_data = copy.deepcopy(sensitivity_batch_data)\n",
    "    max_sensitivity_batch_data = copy.deepcopy(sensitivity_batch_data)\n",
    "\n",
    "    #for safety we iterate again, since we otherwise fill in the zero's that are meant as NANs\n",
    "    for sample_index, data_length in enumerate(sensitivity_batch_data_length):\n",
    "        min_sensitivity_batch_data[sample_index,:data_length, covariate_index] = batch_min\n",
    "        max_sensitivity_batch_data[sample_index,:data_length, covariate_index] = batch_max\n",
    "\n",
    "    min_sensitivity_batch_data = min_sensitivity_batch_data.to(DEVICE)\n",
    "    max_sensitivity_batch_data = max_sensitivity_batch_data.to(DEVICE)\n",
    "    sensitivity_batch_data_length = sensitivity_batch_data_length.to(DEVICE)\n",
    "\n",
    "    #for safety we iterate again, because the previous iteration might be absorbed by the one above that\n",
    "    for sample_index, data_length in enumerate(sensitivity_batch_data_length):\n",
    "        for cause_index in range(NUM_CAUSES):\n",
    "            evaluation_time = min(int(data_length.item()) + delta, MAX_LENGTH)\n",
    "\n",
    "            _, max_fht, _ = DDHT(max_sensitivity_batch_data[sample_index].unsqueeze(0), data_length)\n",
    "            _, min_fht, _ = DDHT(min_sensitivity_batch_data[sample_index].unsqueeze(0), data_length)\n",
    "\n",
    "            max_gamma[cause_index, covariate_index] += CIF_K_tau(max_fht[0], cause_index, evaluation_time, data_length, MAX_LENGTH).item()\n",
    "            min_gamma[cause_index, covariate_index] += CIF_K_tau(min_fht[0], cause_index, evaluation_time, data_length, MAX_LENGTH).item()\n",
    "\n",
    "\n",
    "gamma = (1/BATCH_SIZE)*(min_gamma - max_gamma)\n",
    "plot_gamma(gamma, y_lim=[-1, 1])\n",
    "\n",
    "print(\"The following covariate sensitivities are represented:\")\n",
    "for i, covariate in enumerate(allowed_covariates):\n",
    "    print(\"Index %d represents covariate %s\" % (i, covariate))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e97b579e8c0e40da8e2bba439fcd59aed88dbd21d3c026977017f366946867"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
