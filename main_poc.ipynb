{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from poc_data_v2 import PocDataset, display_sample\n",
    "\n",
    "\n",
    "NUM_CASES = 2**8\n",
    "BATCH_SIZE = 2**4\n",
    "\n",
    "poc_raw_dataset = PocDataset(num_cases=NUM_CASES)\n",
    "data_loader = DataLoader(dataset=poc_raw_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        pin_memory=True)\n",
    "\n",
    "batch_data, batch_data_length, batch_event = next(iter(data_loader))\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the longitudional data\n",
    "display_sample(batch_data[0], batch_data_length[0], batch_event[0])\n",
    "display_sample(batch_data[1], batch_data_length[1], batch_event[1])\n",
    "display_sample(batch_data[2], batch_data_length[2], batch_event[2])\n",
    "display_sample(batch_data[3], batch_data_length[3], batch_event[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poc_raw_dataset = PocDataset(num_cases=1)\n",
    "test_data_loader = DataLoader(dataset=test_poc_raw_dataset,batch_size=1,pin_memory=True)\n",
    "test_batch_data, test_batch_data_length, test_batch_event = next(iter(test_data_loader))\n",
    "\n",
    "display_sample(test_batch_data[0], test_batch_data_length[0], test_batch_event[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dynamicDeepHit import EncoderRNN, AttnDecoderRNN, CauseSpecificSubnetwork, DynamicDeepHit\n",
    "from losses_v2 import loss_1_batch, loss_2_batch, loss_3_batch\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "LEARNING_RATE_ENCODER = 0.0001\n",
    "LEARNING_RATE_DECODER = 0.0001\n",
    "LEARNING_RATE_CAUSESS = 0.00005\n",
    "\n",
    "LOSS_1_AMPLIFIER = 1\n",
    "LOSS_2_AMPLIFIER = 1\n",
    "LOSS_3_AMPLIFIER = 1\n",
    "\n",
    "RUN_VALIDATION_ROUND = False\n",
    "VAL_NUM_CASES_RUNTIME = 2**3\n",
    "\n",
    "input_size = 5\n",
    "output_size = input_size\n",
    "MAX_LENGTH = 36\n",
    "NUM_CAUSES = 3\n",
    "hidden_size_encoder = 512\n",
    "hidden_size_attention = 512\n",
    "fc_size_encoder = 512\n",
    "SIGMA = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "encoder = EncoderRNN(input_size, hidden_size_encoder, fc_size_encoder).to(DEVICE)\n",
    "decoder = AttnDecoderRNN(hidden_size_encoder, hidden_size_attention, output_size).to(DEVICE)\n",
    "causess = CauseSpecificSubnetwork(hidden_size_encoder, input_size, MAX_LENGTH, NUM_CAUSES).to(DEVICE)\n",
    "DDHT = DynamicDeepHit(encoder, decoder, causess, MAX_LENGTH, DEVICE)\n",
    "\n",
    "# intialize optimizer\n",
    "optimizer_encoder = Adam(encoder.parameters(), lr=LEARNING_RATE_ENCODER)\n",
    "optimizer_decoder = Adam(decoder.parameters(), lr=LEARNING_RATE_DECODER)\n",
    "optimizer_causess = Adam(causess.parameters(), lr=LEARNING_RATE_CAUSESS)\n",
    " \n",
    "# initialize loss\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "\n",
    "# start training\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_number, data in enumerate(test_data_loader):\n",
    "\n",
    "    batch_loss = 0\n",
    "\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    optimizer_causess.zero_grad()\n",
    "\n",
    "    batch_data, batch_data_length, batch_event = data\n",
    "    batch_data = batch_data.to(DEVICE)\n",
    "    batch_data_length = batch_data_length.to(DEVICE)\n",
    "    batch_event = batch_event.to(DEVICE)\n",
    "    \n",
    "    output_batch, first_hitting_time_batch = DDHT(batch_data, batch_data_length)\n",
    "\n",
    "    loss1 = loss_1_batch(first_hitting_time_batch, batch_event, batch_data_length, MAX_LENGTH)\n",
    "    loss2 = loss_2_batch(first_hitting_time_batch, batch_event, batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)\n",
    "    loss3 = loss_3_batch(output_batch, batch_data.detach())\n",
    "\n",
    "    batch_loss = loss1 + loss2 + loss3\n",
    "    batch_loss.backward()\n",
    "\n",
    "    epoch_loss += batch_loss.detach()\n",
    "\n",
    "    writer.add_scalar('train_loss1', loss1.item(), epoch*len(data_loader) + batch_number)\n",
    "    writer.add_scalar('train_loss2', loss2.item(), epoch*len(data_loader) + batch_number)\n",
    "    writer.add_scalar('train_loss3', loss3.item(), epoch*len(data_loader) + batch_number)\n",
    "\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_decoder.step()\n",
    "    optimizer_causess.step()\n",
    "\n",
    "  if RUN_VALIDATION_ROUND:\n",
    "    # validating round\n",
    "    DDHT.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      val_poc_raw_dataset = PocDataset(num_cases=VAL_NUM_CASES_RUNTIME)\n",
    "      val_data_loader = torch.utils.data.DataLoader(val_poc_raw_dataset,batch_size=VAL_NUM_CASES_RUNTIME)\n",
    "      val_batch_data, val_data_length, val_batch_event = next(iter(val_data_loader))\n",
    "      val_batch_data = val_batch_data.to(DEVICE)\n",
    "      val_data_length = val_data_length.to(DEVICE)\n",
    "      val_batch_event = val_batch_event.to(DEVICE)\n",
    "\n",
    "      val_output_batch, val_first_hitting_time_batch = DDHT(val_batch_data, val_data_length)\n",
    "\n",
    "      val_loss1 = loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_data_length, MAX_LENGTH)/VAL_NUM_CASES_RUNTIME\n",
    "      val_loss2 = loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "      val_loss3 = loss_3_batch(val_output_batch, val_batch_data.detach())/VAL_NUM_CASES_RUNTIME\n",
    "\n",
    "      writer.add_scalar('val_loss1', val_loss1.item(), epoch)\n",
    "      writer.add_scalar('val_loss2', val_loss2.item(), epoch)\n",
    "      writer.add_scalar('val_loss3', val_loss3.item(), epoch)\n",
    "      writer.add_scalar('val_epoch_loss', val_loss1.item() + val_loss2.item() + val_loss3.item(), epoch)\n",
    "\n",
    "    DDHT.train()\n",
    "    # end validating round\n",
    "\n",
    "  writer.add_scalar('train_epoch_loss', epoch_loss.item(), epoch)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"C:/Users/marij/Desktop/THESIS ECO/DDHT/DDHT_pytorch/models/model_3.pth\"\n",
    "#DDHT.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDHT.eval();\n",
    "display_sample(test_batch_data[0], test_batch_data_length[0], test_batch_event[0])\n",
    "test_encoder_output_vector, test_first_hitting_time = DDHT(test_batch_data.to(DEVICE), test_batch_data_length.to(DEVICE))\n",
    "print(test_encoder_output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.round(test_first_hitting_time * 10**2) / (10**2))\n",
    "print(test_first_hitting_time.argmax().item())\n",
    "print(test_first_hitting_time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "ax1.bar([i for i in range(MAX_LENGTH)], test_first_hitting_time.squeeze()[:MAX_LENGTH].cpu().detach().numpy())\n",
    "ax2.bar([i for i in range(MAX_LENGTH)], test_first_hitting_time.squeeze()[MAX_LENGTH:2*MAX_LENGTH].cpu().detach().numpy())\n",
    "ax3.bar([i for i in range(MAX_LENGTH)], test_first_hitting_time.squeeze()[2*MAX_LENGTH:].cpu().detach().numpy())\n",
    "ax1.set_title(\"event 1\")\n",
    "ax2.set_title(\"event 2\")\n",
    "ax3.set_title(\"event 3\")\n",
    "ax1.set_ylim([0,1]);\n",
    "ax2.set_ylim([0,1]);\n",
    "ax3.set_ylim([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "ax1.bar([i for i in range(MAX_LENGTH)], torch.cumsum(test_first_hitting_time.squeeze()[:MAX_LENGTH], dim=0).cpu().detach().numpy())\n",
    "ax2.bar([i for i in range(MAX_LENGTH)], torch.cumsum(test_first_hitting_time.squeeze()[MAX_LENGTH:2*MAX_LENGTH], dim=0).cpu().detach().numpy())\n",
    "ax3.bar([i for i in range(MAX_LENGTH)], torch.cumsum(test_first_hitting_time.squeeze()[2*MAX_LENGTH:], dim=0).cpu().detach().numpy())\n",
    "ax1.set_title(\"event 1\")\n",
    "ax2.set_title(\"event 2\")\n",
    "ax3.set_title(\"event 3\")\n",
    "ax1.set_ylim([0,1]);\n",
    "ax2.set_ylim([0,1]);\n",
    "ax3.set_ylim([0,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poc_data import PocDataset\n",
    "\n",
    "VAL_NUM_CASES = 2**6\n",
    "\n",
    "val_poc_raw_dataset = PocDataset(num_cases=VAL_NUM_CASES)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_poc_raw_dataset,batch_size=2**6)\n",
    "val_input_batch, val_event_batch, val_time_to_event_batch, _ = next(iter(val_data_loader))\n",
    "val_input_batch = val_input_batch.to(DEVICE)\n",
    "val_event_batch = val_event_batch.to(DEVICE)\n",
    "val_time_to_event_batch = val_time_to_event_batch.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDHT.eval()\n",
    "\n",
    "val_output_batch, val_first_hitting_time_batch = DDHT(val_input_batch, val_time_to_event_batch)\n",
    "val_loss1 = loss_1_batch(val_first_hitting_time_batch, val_event_batch, val_time_to_event_batch, MAX_LENGTH)/VAL_NUM_CASES\n",
    "val_loss2 = loss_2_batch(val_first_hitting_time_batch, val_event_batch, val_time_to_event_batch, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES\n",
    "val_loss3 = loss_3_batch(val_output_batch, val_input_batch.detach())/VAL_NUM_CASES\n",
    "\n",
    "\n",
    "print(\"val_loss1=\", val_loss1.item())\n",
    "print(\"val_loss2=\", val_loss2.item())\n",
    "print(\"val_loss3=\", val_loss3.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records:\n",
    "val_loss1= 0.09860126674175262\n",
    "val_loss2= 16.080060958862305\n",
    "val_loss3= 690.994384765625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/marij/Desktop/THESIS ECO/DDHT/DDHT_pytorch/models/model_3.pth\"\n",
    "torch.save(DDHT.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e97b579e8c0e40da8e2bba439fcd59aed88dbd21d3c026977017f366946867"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
