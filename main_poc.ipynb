{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "SRC_PATH = '/content/gdrive/MyDrive/MP FEB/Colab'\n",
    "sys.path.append(SRC_PATH)\n",
    "\n",
    "!pip install wandb -qqq\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"DDHT_poc\", \n",
    "    # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    #name=\"experiment 1\"\n",
    "    # Track hyperparameters and run metadata\n",
    "    #config={\n",
    "      #\"learning_rate\": 0.02,\n",
    "      #\"architecture\": \"CNN\",\n",
    "      #\"dataset\": \"CIFAR-100\",\n",
    "      #\"epochs\": 10,}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from poc_data import PocDataset, display_sample\n",
    "\n",
    "\n",
    "NUM_CASES = 2**6\n",
    "BATCH_SIZE = 2**4\n",
    "\n",
    "poc_raw_dataset = PocDataset(num_cases=NUM_CASES, augment=True)\n",
    "data_loader = DataLoader(dataset=poc_raw_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True)\n",
    "\n",
    "batch_data, batch_data_length, batch_event, batch_tte, _ = next(iter(data_loader))\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_data_length.shape)\n",
    "print(batch_event.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the longitudional data\n",
    "display_sample(batch_data[0], batch_data_length[0], batch_event[0])\n",
    "display_sample(batch_data[1], batch_data_length[1], batch_event[1])\n",
    "display_sample(batch_data[2], batch_data_length[2], batch_event[2])\n",
    "display_sample(batch_data[3], batch_data_length[3], batch_event[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poc_raw_dataset = PocDataset(num_cases=1)\n",
    "test_data_loader = DataLoader(dataset=test_poc_raw_dataset,batch_size=1,pin_memory=True)\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_tte, test_meta = next(iter(test_data_loader))\n",
    "display_sample(test_batch_data[0], test_batch_data_length[0], test_batch_event[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from dynamicDeepHit import EncoderRNN, AttnDecoderRNN, CauseSpecificSubnetwork, DynamicDeepHit\n",
    "from losses import loss_1_batch, loss_2_batch, loss_3_batch\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "LEARNING_RATE_ENCODER = 0.001\n",
    "LEARNING_RATE_DECODER = 0.001\n",
    "LEARNING_RATE_CAUSESS = 0.0005\n",
    "\n",
    "LOSS_1_AMPLIFIER = 1\n",
    "LOSS_2_AMPLIFIER = 1\n",
    "LOSS_3_AMPLIFIER = 1\n",
    "\n",
    "RUN_VALIDATION_ROUND = False\n",
    "VAL_NUM_CASES_RUNTIME = BATCH_SIZE\n",
    "\n",
    "input_size = 5\n",
    "output_size = input_size\n",
    "MAX_LENGTH = 2*12\n",
    "NUM_CAUSES = 3\n",
    "hidden_size_encoder = 256\n",
    "hidden_size_attention = 512\n",
    "fc_size_encoder = 512\n",
    "SIGMA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "encoder = EncoderRNN(input_size, hidden_size_encoder, fc_size_encoder).to(DEVICE)\n",
    "decoder = AttnDecoderRNN(hidden_size_encoder, hidden_size_attention, output_size).to(DEVICE)\n",
    "causess = CauseSpecificSubnetwork(hidden_size_encoder, input_size, MAX_LENGTH, NUM_CAUSES).to(DEVICE)\n",
    "DDHT = DynamicDeepHit(encoder, decoder, causess, MAX_LENGTH, DEVICE)\n",
    "\n",
    "# intialize optimizer\n",
    "optimizer_encoder = Adam(encoder.parameters(), lr=LEARNING_RATE_ENCODER)\n",
    "optimizer_decoder = Adam(decoder.parameters(), lr=LEARNING_RATE_DECODER)\n",
    "optimizer_causess = Adam(causess.parameters(), lr=LEARNING_RATE_CAUSESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/content/gdrive/MyDrive/MP FEB/Colab/models/model_v14_2048.pth\"\n",
    "\n",
    "# start training\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for batch_number, data in enumerate(data_loader):\n",
    "\n",
    "    batch_loss = 0\n",
    "\n",
    "    optimizer_encoder.zero_grad()\n",
    "    optimizer_decoder.zero_grad()\n",
    "    optimizer_causess.zero_grad()\n",
    "\n",
    "    batch_data, batch_data_length, batch_event, batch_tte, _ = data\n",
    "    batch_data = batch_data.to(DEVICE)\n",
    "    batch_data_length = batch_data_length.to(DEVICE)\n",
    "    batch_event = batch_event.to(DEVICE)\n",
    "    batch_tte = batch_tte.to(DEVICE)\n",
    "    \n",
    "    output_batch, first_hitting_time_batch = DDHT(batch_data, batch_data_length)\n",
    "\n",
    "    loss1 = LOSS_1_AMPLIFIER*loss_1_batch(first_hitting_time_batch, batch_event, batch_tte, batch_data_length, MAX_LENGTH, DEVICE)\n",
    "    loss2 = LOSS_2_AMPLIFIER*loss_2_batch(first_hitting_time_batch, batch_event, batch_tte, batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)\n",
    "    loss3 = LOSS_3_AMPLIFIER*loss_3_batch(output_batch, batch_data.detach())\n",
    "\n",
    "    batch_loss = loss1 + loss2 + loss3\n",
    "    batch_loss.backward()\n",
    "\n",
    "    epoch_loss += batch_loss.detach()\n",
    "\n",
    "    wandb.log({\"train_loss1\": loss1.item(), \"train_loss2\": loss2.item(), \"train_loss3\": loss3.item()})\n",
    "\n",
    "    optimizer_encoder.step()\n",
    "    optimizer_decoder.step()\n",
    "    optimizer_causess.step()\n",
    "\n",
    "    if batch_number % 2**8 == 0:\n",
    "      torch.save(DDHT.state_dict(), PATH)\n",
    "\n",
    "\n",
    "  if RUN_VALIDATION_ROUND:\n",
    "    # validating round\n",
    "    DDHT.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      val_poc_raw_dataset = PocDataset(num_cases=VAL_NUM_CASES_RUNTIME)\n",
    "      val_data_loader = torch.utils.data.DataLoader(val_poc_raw_dataset,batch_size=VAL_NUM_CASES_RUNTIME)\n",
    "      val_batch_data, val_data_length, val_batch_event, val_batch_tte, _ = next(iter(val_data_loader))\n",
    "      val_batch_data = val_batch_data.to(DEVICE)\n",
    "      val_data_length = val_data_length.to(DEVICE)\n",
    "      val_batch_event = val_batch_event.to(DEVICE)\n",
    "      val_batch_tte = val_batch_tte.to(DEVICE)\n",
    "\n",
    "      val_output_batch, val_first_hitting_time_batch = DDHT(val_batch_data, val_data_length)\n",
    "\n",
    "      val_loss1 = LOSS_1_AMPLIFIER*loss_1_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, val_batch_data_length, MAX_LENGTH, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "      val_loss2 = LOSS_2_AMPLIFIER*loss_2_batch(val_first_hitting_time_batch, val_batch_event, val_batch_tte, val_batch_data_length, NUM_CAUSES, MAX_LENGTH, SIGMA, DEVICE)/VAL_NUM_CASES_RUNTIME\n",
    "      val_loss3 = LOSS_3_AMPLIFIER*loss_3_batch(val_output_batch, val_batch_data.detach())/VAL_NUM_CASES_RUNTIME\n",
    "\n",
    "      wandb.log({\"val_loss1\": val_loss1.item(), \"val_loss2\": val_loss2.item(), \"val_loss3\": val_loss3.item()})\n",
    "      wandb.log({\"train_epoch_loss\" : epoch_loss.item(), \"val_epoch_loss\" : val_loss1.item() + val_loss2.item() + val_loss3.item(),\"epoch\": epoch})\n",
    "\n",
    "    DDHT.train()\n",
    "    # end validating round\n",
    "\n",
    "  torch.save(DDHT.state_dict(), PATH)\n",
    "\n",
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/marij/Desktop/THESIS ECO/DDHT/DDHT_pytorch/models/model_v14_2048.pth\"\n",
    "#torch.save(DDHT.state_dict(), PATH)\n",
    "DDHT.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring our model - sampling a new test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_fht_and_cif\n",
    "from losses import CIF_K_tau\n",
    "\n",
    "DDHT.eval()\n",
    "test_data_loader = PocDataset(num_cases=1, test_set=True, repays=False)\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte, test_meta = next(iter(test_data_loader))\n",
    "test_batch_data = test_batch_data.unsqueeze(0).to(DEVICE)\n",
    "test_batch_data_length = test_batch_data_length.unsqueeze(0).to(DEVICE)\n",
    "test_batch_event = test_batch_event.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "test_output, test_first_hitting_time = DDHT(test_batch_data, test_batch_data_length)\n",
    "print(\"sample has length %d\" % test_batch_data_length[0])\n",
    "print(\"sample will experience event %d at time %d, but shows event %d\" % (test_meta['ground_truth_event'], test_meta['age'], test_batch_event[0]))\n",
    "\n",
    "\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax().item()\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction, model_tte_prediction + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 0, 24, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 1, 24, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 2, 24, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[0], test_batch_data_length[0], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte, test_meta = next(iter(test_data_loader))\n",
    "test_batch_data_length = test_batch_data_length.unsqueeze(0).to(DEVICE)\n",
    "test_batch_event = test_batch_event.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "masked_data = copy.deepcopy(test_batch_data.unsqueeze(0))\n",
    "\n",
    "#masked_data[:,:,0] = 0\n",
    "#masked_data[:,:,1] = 0\n",
    "#masked_data[:,:,2] = 0\n",
    "#masked_data[:,:,3] = 0\n",
    "#masked_data[:,:,4] = 0\n",
    "\n",
    "masked_data = masked_data.to(DEVICE)\n",
    "\n",
    "test_output, test_first_hitting_time = DDHT(masked_data, test_batch_data_length)\n",
    "print(\"sample has length %d\" % test_batch_data_length[0])\n",
    "print(\"sample will experience event %d at time %d, but shows event %d\" % (test_meta['ground_truth_event'], test_meta['age'], test_batch_event[0]))\n",
    "\n",
    "\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax().item()\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction, model_tte_prediction + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 0, 24, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 1, 24, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 2, 24, test_batch_data_length[0], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[0], test_batch_data_length[0], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability that this loan will prepay, default or repay in the comming \"delta\" months?\n",
    "\n",
    "delta = 6  #This is variable\n",
    "\n",
    "evaluation_time = min(int(test_batch_data_length[0].item()) + delta, MAX_LENGTH)\n",
    "print(\"In the comming %d months, the probability that a specific event happens is:\" % delta)\n",
    "\n",
    "p_ev0 = CIF_K_tau(test_first_hitting_time[0], 0, evaluation_time, test_batch_data_length[0], MAX_LENGTH).item()\n",
    "p_ev1 = CIF_K_tau(test_first_hitting_time[0], 1, evaluation_time, test_batch_data_length[0], MAX_LENGTH).item()\n",
    "p_ev2 = CIF_K_tau(test_first_hitting_time[0], 2, evaluation_time, test_batch_data_length[0], MAX_LENGTH).item()\n",
    "\n",
    "print(\"probability a prepay happens = %.3f\" % p_ev0)\n",
    "print(\"probability a default happens = %.3f\" % p_ev1)\n",
    "print(\"probability a full repay happens = %.3f\" % p_ev2)\n",
    "\n",
    "sum = p_ev0 + p_ev1 + p_ev2\n",
    "print(\"The probability anything happens = %.3f\" % sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability that this loan will prepay, default or repay with less data\n",
    "test_batch_data, test_batch_data_length, test_batch_event, test_batch_tte, test_meta = next(iter(test_data_loader))\n",
    "\n",
    "delta = 5 #This is variable\n",
    "shortened_length = test_batch_data_length - delta\n",
    "shortened_test_batch_data = copy.deepcopy(test_batch_data)\n",
    "shortened_test_batch_data[shortened_length:test_batch_data_length] = torch.zeros(delta, input_size)\n",
    "shortened_test_batch_data_length = copy.deepcopy(shortened_length)\n",
    "\n",
    "shortened_test_batch_data = shortened_test_batch_data.unsqueeze(0).to(DEVICE)\n",
    "shortened_test_batch_data_length = shortened_test_batch_data_length.unsqueeze(0).to(DEVICE)\n",
    "test_batch_event = test_batch_event.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "test_output, test_first_hitting_time = DDHT(shortened_test_batch_data, shortened_test_batch_data_length)\n",
    "print(\"sample has length %d, but we concatenated to %d\" % (test_batch_data_length, shortened_test_batch_data_length[0]))\n",
    "print(\"sample will experience event %d at time %d, but shows event %d\" % (test_meta['ground_truth_event'], test_meta['age'], test_batch_event[0]))\n",
    "\n",
    "\n",
    "test_first_hitting_time_argmax = test_first_hitting_time.argmax().item()\n",
    "model_event_prediction = test_first_hitting_time_argmax // MAX_LENGTH\n",
    "model_tte_prediction = test_first_hitting_time_argmax % MAX_LENGTH\n",
    "print(\"the model predicts the event %d at time %d\" % (model_event_prediction, model_tte_prediction + 1))\n",
    "\n",
    "print(\"probability of prepay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 0, 24, shortened_test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of default event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 1, 24, shortened_test_batch_data_length[0], MAX_LENGTH).item())\n",
    "print(\"probability of full repay event = %.2f\" % CIF_K_tau(test_first_hitting_time[0], 2, 24, shortened_test_batch_data_length[0], MAX_LENGTH).item())\n",
    "\n",
    "plot_fht_and_cif(test_first_hitting_time[0], shortened_test_batch_data_length[0], MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_gamma\n",
    "\n",
    "SENSITIVITY_BATCH_SIZE = 2**8\n",
    "\n",
    "sensitivity_poc_dataset = PocDataset(num_cases=SENSITIVITY_BATCH_SIZE, test_set=True, repays=True, augment=False)\n",
    "sensitivity_data_loader = DataLoader(dataset=sensitivity_poc_dataset, batch_size=SENSITIVITY_BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "delta = 6 #This is variable\n",
    "\n",
    "#do 2 placeholders for the results\n",
    "min_gamma = torch.zeros(NUM_CAUSES, input_size)\n",
    "max_gamma = torch.zeros(NUM_CAUSES, input_size)\n",
    "\n",
    "for covariate_index in range(input_size):\n",
    "    #Probability that this loan will prepay, default or repay with less data\n",
    "    sensitivity_batch_data, sensitivity_batch_data_length, sensitivity_batch_event, sensitivity_batch_tte, sensitivity_meta = next(iter(sensitivity_data_loader))\n",
    "\n",
    "    #assuming it's minimum & maximum from the whole batch, not from a single sample\n",
    "    batch_min = torch.tensor(float('Inf'))\n",
    "    batch_max = (-1)*torch.tensor(float('Inf'))\n",
    "\n",
    "    #we have to iterate over it otherwise, we catch the zero's that are meant as NANs in our case\n",
    "    for sample, data_length in zip(sensitivity_batch_data, sensitivity_batch_data_length):\n",
    "        sample_min = torch.min(sample[:data_length,covariate_index])\n",
    "        sample_max = torch.max(sample[:data_length,covariate_index])\n",
    "\n",
    "        if sample_min < batch_min:\n",
    "            batch_min = sample_min\n",
    "\n",
    "        if sample_max > batch_max:\n",
    "            batch_max = sample_max\n",
    "\n",
    "    min_sensitivity_batch_data = copy.deepcopy(sensitivity_batch_data)\n",
    "    max_sensitivity_batch_data = copy.deepcopy(sensitivity_batch_data)\n",
    "\n",
    "    #for safety we iterate again, since we otherwise fill in the zero's that are meant as NANs\n",
    "    for sample_index, data_length in enumerate(sensitivity_batch_data_length):\n",
    "        min_sensitivity_batch_data[sample_index,:data_length, covariate_index] = batch_min\n",
    "        max_sensitivity_batch_data[sample_index,:data_length, covariate_index] = batch_max\n",
    "\n",
    "    min_sensitivity_batch_data = min_sensitivity_batch_data.to(DEVICE)\n",
    "    max_sensitivity_batch_data = max_sensitivity_batch_data.to(DEVICE)\n",
    "    sensitivity_batch_data_length = sensitivity_batch_data_length.to(DEVICE)\n",
    "\n",
    "    #for safety we iterate again, because the previous iteration might be absorbed by the one above that\n",
    "    for sample_index, data_length in enumerate(sensitivity_batch_data_length):\n",
    "        for cause_index in range(NUM_CAUSES):\n",
    "            evaluation_time = min(int(data_length.item()) + delta, MAX_LENGTH)\n",
    "\n",
    "            _, max_fht = DDHT(max_sensitivity_batch_data[sample_index].unsqueeze(0), data_length)\n",
    "            _, min_fht = DDHT(min_sensitivity_batch_data[sample_index].unsqueeze(0), data_length)\n",
    "\n",
    "            max_gamma[cause_index, covariate_index] += CIF_K_tau(max_fht[0], cause_index, evaluation_time, data_length, MAX_LENGTH).item()\n",
    "            min_gamma[cause_index, covariate_index] += CIF_K_tau(min_fht[0], cause_index, evaluation_time, data_length, MAX_LENGTH).item()\n",
    "\n",
    "\n",
    "gamma = (1/SENSITIVITY_BATCH_SIZE)*(min_gamma - max_gamma)\n",
    "plot_gamma(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e97b579e8c0e40da8e2bba439fcd59aed88dbd21d3c026977017f366946867"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
